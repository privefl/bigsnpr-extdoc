[["index.html", "bigsnpr &amp; bigstatsr Extended documentation [WORK IN PROGRESS] About License Author Contact", " bigsnpr &amp; bigstatsr Extended documentation [WORK IN PROGRESS] Florian Privé 2022-07-18 About License This material is licensed under the Creative Commons Attribution-ShareAlike 3.0 License. Author Florian Privé is a postdoc in predictive human genetics, fond of Data Science and an R(cpp) enthusiast. He is also the founder and former organizer of the Grenoble R user group. You can find him on Twitter and GitHub as @privefl and on Stack Overflow as F. Privé. Contact If you want me to add or clarify some content in this documentation, please open an issue on the GitHub repository of this documentation. If you have bug reports or questions specifically on functions of the packages, please open an issue on the corresponding packages repository. I will always redirect you to GitHub issues if you email me about the packages, so that others can benefit from our discussion. "],["introduction.html", "Chapter 1 Introduction 1.1 Main motivation for developing {bigstatsr} and {bigsnpr} 1.2 Features 1.3 Installation 1.4 Correct spellings", " Chapter 1 Introduction 1.1 Main motivation for developing {bigstatsr} and {bigsnpr} The main motivation was for me to be able to run all my analyses within . I was frustrated by having to use all these different software, with different input formats, and requiring text files for parameters. This made it hard for me to build a chain of analyses, to perform some exploratory analyses, or to use familiar packages. Also, I wanted to develop new methods, which seemed very hard to do without using a simple matrix-like format. Thus I started developing package {bigsnpr} at the beginning of my thesis. At some point, I realized that many functions (to perform e.g. GWAS, PCA, summary statistics) were not really specific to genotype data. Indeed, a TWAS or an EWAS are not conceptually very different from a GWAS; one can also perform PCA on e.g. DNA methylation data. Therefore I decided to move all these functions that could be used on any data stored as a matrix and build a new package, {bigstatsr}. This is why there are two packages, where {bigstatsr} can basically be used by any field using matrices, while {bigsnpr} provides some tools rather specific to genotype data, largely building on top of {bigstatsr}. The initial description of the two packages is available in Privé et al. (2018). To know which function belongs to which package: functions starting with big_ belongs to {bigstatsr} while functions starting with snp_ or bed_ belongs to {bigsnpr}. 1.2 Features There are now many things implemented in the packages. You can find a comprehensive list of available functions on the package website of {bigstatsr} and of {bigsnpr}. The next table presents an overview of common genetic analyses that are already implemented in {bigstatsr} and {bigsnpr}. This listing is inspired from table 1 of Visscher et al. (2017) The Role of GWAS SNP Arrays in Human Genetic Discoveries. Analysis Available in {bigstatsr} and {bigsnpr} Still missing Citations GWAS linear and logistic - mixed models - rare variant association Privé et al. (2018) Genome-wide assessment of LD - sparse correlation matrix - optimal LD splitting Privé et al. (2018) Privé (2021) Estimation of SNP heritability LDpred2-auto &amp; LD score regression need proper validation Privé, Arbel, and Vilhjálmsson (2020) Estimation of polygenicity LDpred2-auto need proper validation Privé, Arbel, and Vilhjálmsson (2020) Estimation of genetic correlation need to extend LDpred2-auto Polygenic risk scores - penalized regressions - (stacked) C+T - LDpred2 Privé, Aschard, and Blum (2019) Privé et al. (2019) Privé, Arbel, and Vilhjálmsson (2020) Mendelian randomization completely missing Population structure - principal component analysis - ancestry inference - fixation index (\\(F_{ST}\\)) - local adaptation Privé et al. (2018) &amp; Privé, Luu, Blum, et al. (2020) Privé et al. (2021) Privé, Luu, Blum, et al. (2020) Privé, Luu, Vilhjálmsson, et al. (2020) Fine-mapping completely missing Miscellaneous - integration with PLINK - format conversion - imputation of genotyped variants - matrix operations - summaries Privé et al. (2018) 1.3 Installation Both packages are available on CRAN, so you can use install.packages(): install.packages(&quot;bigstatsr&quot;) install.packages(&quot;bigsnpr&quot;) To install the latest versions (from GitHub), you can use {remotes}: # install.packages(&quot;remotes&quot;) remotes::install_github(&quot;privefl/bigstatsr&quot;) remotes::install_github(&quot;privefl/bigsnpr&quot;) As an alternative to {remotes}, you can also try {pak}, which gives more information and solve other issues. 1.4 Correct spellings A friendly reminder:The correct spelling is - bigstatsr  not bigstatr / BIGstatsR - bigsnpr  not BIGsnpR / bigSNPr - pcadapt  not PCAdapt - LDpred  not LDPredThank youThe kittens thank you too pic.twitter.com/S8wyE4G6BG  Florian Privé (remote postdoc) ((privefl?)) November 18, 2020 "],["inputs-and-formats.html", "Chapter 2 Inputs and formats 2.1 In {bigstatsr} 2.2 In {bigsnpr} 2.3 Getting a FBM or bigSNP object", " Chapter 2 Inputs and formats 2.1 In {bigstatsr} The format provided in package {bigstatsr} is called a Filebacked Big Matrix (FBM). It is an on-disk matrix format which is accessed through memory-mapping. Memory-mapping accesses the elements you need from disk to memory when needed. The second time you access the same elements, they are directly accessed from memory while there is enough memory for storing them. When no more memory is available, the OS frees some of the memory to make room for new elements you want to access. Therefore, try to always perform as many operations on a subset of elements since data is accessed from disk only once. All the elements of one FBM have the same type; supported types are: \"double\" (the default, double precision  64 bits) \"float\" (single precision  32 bits) \"integer\" (signed, so between \\(-2^{31}\\) and \\(2^{31} - 1\\)) \"unsigned short\": can store integer values from \\(0\\) to \\(65535\\). \"raw\" or \"unsigned char\": can store integer values from \\(0\\) to \\(255\\). It is the basis for class FBM.code256 in order to access 256 arbitrary different numeric values. It is used in package {bigsnpr} (see below). Standard matrix accessors are implemented for FBMs, so you can e.g. access the first column of X using X[, 1]. You can access the whole FBM as an R matrix in memory using X[]. However, if the matrix is too large to fit in memory, you should always access only a subset of columns. Note that the elements of the FBM are stored column-wise (as for a standard R matrix). Therefore, be careful not to access a subset of rows, since it will read non-contiguous elements from the whole matrix from disk. 2.2 In {bigsnpr} Package {bigsnpr} uses a class called bigSNP for representing SNP data. A bigSNP object is merely a list with the following elements: $genotypes: A FBM.code256. Rows are samples and columns are variants. This stores genotype calls or dosages (rounded to 2 decimal places). $fam: A data.frame with some information on the individuals. $map: A data.frame with some information on the variants. The code used in class FBM.code256 for imputed data is e.g. bigsnpr::CODE_DOSAGE #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 #&gt; [15] 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 #&gt; [29] 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 #&gt; [43] 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] where the first four elements are used to store genotype calls, the next three to store imputed allele counts, and the next 201 values to store dosages rounded to 2 decimal places. This allows for handling many types of data while storing each elements using one byte only (x4 compared to bed files, but /8 compared to doubles). Since v1.0, package {bigsnpr} also provides functions for directly working on bed files with a few missing values (Privé, Luu, Blum, et al. 2020). If there is a demand for it, I might extend functions in {bigsnpr} to handle more types of FBMs than only FBM.code256. We have started talking about this in this issue. 2.3 Getting a FBM or bigSNP object The easiest way to get an FBM is to use the constructor function FBM() or the converter as_FBM(). To read an FBM from a large text file, you can use function big_read() (see this vignette). To read a bigSNP object from bed/bim/fam files, you can use functions snp_readBed() and snp_readBed2() (the second one allows using only a subset of individuals/variants and using parallelism). To read BGEN files, you can use function snp_readBGEN(). This function takes around 40 minutes to read 1M variants for 400K individuals using 15 cores. Note that this function works only for BGEN V1.2 with probabilities stored as 8 bits (which is the case of e.g. the UK Biobank files, see this issue). To read any format used in genetics, you can always convert blocks of the data to text files using PLINK, read these using bigreadr::fread2() and filling a part of the resulting FBM. [TODO: export the code to convert from RICOPILI imputed data] "],["preprocessing.html", "Chapter 3 Preprocessing 3.1 Conversion and quality control of PLINK files 3.2 Imputation 3.3 Exercise: preprocessing", " Chapter 3 Preprocessing In this section, I am including conversion, quality control and imputation. 3.1 Conversion and quality control of PLINK files PLINK is very efficient for conversion and quality control of multiple formats, so I have decided just using it. In {bigsnpr}, I provide some wrappers to PLINK for ease of use: download_plink() and download_plink2() for downloading the latest stable versions of PLINK 1.9 and 2.0. snp_plinkQC() for quality control (QC) and conversion to bed/bim/fam. snp_plinkKINGQC() for QC on relatedness based on KING-robust kinship estimator. Using make.bed = FALSE allows for computing related pairs only, i.e. reporting a data frame without producing new bed/bim/fam files. snp_plinkIBDQC() for QC based on identity-by-descent (IBD) computed by PLINK using its method-of-moments. snp_plinkRmSamples() for producing new PLINK files after having removed some individuals. For any other PLINK function, I recommend calling PLINK directly from thanks to system calls and package {glue}, e.g. plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --version&quot; )) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) 3.2 Imputation Note that most functions from {bigstatsr} and {bigsnpr} dont handle missing values. Simple imputation (e.g. by the mean) of a double FBM can be performed by blocks using e.g. as explained in this vignette. In {bigsnpr}, to perform simple imputation of genotyped data, you can use snp_fastImputeSimple(). You can also use snp_fastImpute() that uses XGBoost models to impute genotyped data (slower but still fast enough, described in Privé et al. (2018)). 3.3 Exercise: preprocessing For the exercises, we will use the data provided in Reed et al. (2015). This can be downloaded using url &lt;- &quot;https://www.mtholyoke.edu/courses/afoulkes/Data/statsTeachR/&quot; sapply(paste0(url, &quot;GWAS_data&quot;, c(&quot;.bed&quot;, &quot;.bim&quot;, &quot;.fam&quot;)), runonce::download_file, dir = &quot;tmp-data&quot;) #&gt; https://www.mtholyoke.edu/courses/afoulkes/Data/statsTeachR/GWAS_data.bed #&gt; &quot;tmp-data/GWAS_data.bed&quot; #&gt; https://www.mtholyoke.edu/courses/afoulkes/Data/statsTeachR/GWAS_data.bim #&gt; &quot;tmp-data/GWAS_data.bim&quot; #&gt; https://www.mtholyoke.edu/courses/afoulkes/Data/statsTeachR/GWAS_data.fam #&gt; &quot;tmp-data/GWAS_data.fam&quot; For some reason, this data is not ordered by chromosome and position; we can use PLINK to get an ordered version of this using library(bigsnpr) plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --bfile tmp-data/GWAS_data&quot;, &quot; --make-bed --out tmp-data/GWAS_data_sorted&quot; )) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) www.cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2022 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data #&gt; --make-bed #&gt; --out tmp-data/GWAS_data_sorted #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 500000 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted.bed + tmp-data/GWAS_data_sorted.bim + #&gt; tmp-data/GWAS_data_sorted.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. As you can see from PLINK output, this data contains 1401 individuals and 500,000 variants, with a few missing values. We can then perform some quality control using bedfile2 &lt;- snp_plinkQC(plink, &quot;tmp-data/GWAS_data_sorted&quot;) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) www.cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2022 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted_QC.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data_sorted #&gt; --geno 0.1 #&gt; --hwe 1e-50 #&gt; --maf 0.01 #&gt; --make-bed #&gt; --mind 0.1 #&gt; --out tmp-data/GWAS_data_sorted_QC #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; 0 people removed due to missing genotype data (--mind). #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 27390 variants removed due to missing genotype data (--geno). #&gt; --hwe: 91 variants removed due to Hardy-Weinberg exact test. #&gt; 67856 variants removed due to minor allele threshold(s) #&gt; (--maf/--max-maf/--mac/--max-mac). #&gt; 404663 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted_QC.bed + #&gt; tmp-data/GWAS_data_sorted_QC.bim + tmp-data/GWAS_data_sorted_QC.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. 404,663 variants are remaining after this quality control; we can then read this data into an R object called bigSNP using (rds &lt;- snp_readBed2(bedfile2, ncores = nb_cores())) #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\Desktop\\\\bigsnpr-extdoc\\\\tmp-data\\\\GWAS_data_sorted_QC.rds&quot; obj.bigsnp &lt;- snp_attach(rds) str(obj.bigsnp, max.level = 2) #&gt; List of 3 #&gt; $ genotypes:Reference class &#39;FBM.code256&#39; [package &quot;bigstatsr&quot;] with 16 fields #&gt; ..and 26 methods, of which 12 are possibly relevant: #&gt; .. add_columns, as.FBM, bm, bm.desc, check_dimensions, #&gt; .. check_write_permissions, copy#envRefClass, initialize, #&gt; .. initialize#FBM, save, show#envRefClass, show#FBM #&gt; $ fam :&#39;data.frame&#39;: 1401 obs. of 6 variables: #&gt; ..$ family.ID : int [1:1401] 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; ..$ sample.ID : int [1:1401] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ paternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ maternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ sex : int [1:1401] 1 2 1 1 1 1 1 2 1 2 ... #&gt; ..$ affection : int [1:1401] 1 1 2 1 2 2 2 1 2 -9 ... #&gt; $ map :&#39;data.frame&#39;: 404663 obs. of 6 variables: #&gt; ..$ chromosome : int [1:404663] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ marker.ID : chr [1:404663] &quot;rs12565286&quot; &quot;rs3094315&quot; &quot;rs2980319&quot; &quot;rs2980300&quot; ... #&gt; ..$ genetic.dist: int [1:404663] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ physical.pos: int [1:404663] 721290 752566 777122 785989 798959 947034 949608 1018704 1041700 1129672 ... #&gt; ..$ allele1 : chr [1:404663] &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; ... #&gt; ..$ allele2 : chr [1:404663] &quot;C&quot; &quot;T&quot; &quot;T&quot; &quot;G&quot; ... #&gt; - attr(*, &quot;class&quot;)= chr &quot;bigSNP&quot; We can store some extra information on the individuals (e.g. some phenotypes): clinical &lt;- bigreadr::fread2( runonce::download_file(paste0(url, &quot;GWAS_clinical.csv&quot;), dir = &quot;tmp-data&quot;)) # Get the same order as for the genotypes # (to match over multiple columns, use `vctrs::vec_match()`) pheno &lt;- clinical[match(obj.bigsnp$fam$family.ID, clinical$FamID), ] # Some verif stopifnot(all.equal(obj.bigsnp$fam$sex, pheno$sex)) # Update the $fam component obj.bigsnp$fam &lt;- cbind(obj.bigsnp$fam, pheno[-c(1, 3)]) Recall that this data contains some missing values; you can get some counts per variant using G &lt;- obj.bigsnp$genotypes counts &lt;- big_counts(G) counts[, 1:8] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 hist(nbNA &lt;- counts[4, ]) We can e.g. perform a quick imputation by the mean using G2 &lt;- snp_fastImputeSimple(G, method = &quot;mean2&quot;, ncores = nb_cores()) big_counts(G2, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 0 0 0 0 0 0 0 0 #&gt; 0.01 0 0 0 0 0 0 0 0 #&gt; 0.02 0 0 0 0 0 0 0 0 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 196 rows ] big_counts(G, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 G still has missing values, but G2 does not. Note that both are using the same data, the difference is that they using a different code to decode the underlying data: G$code256 #&gt; [1] 0 1 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [24] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [47] NA NA NA NA #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] G2$code256 #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 #&gt; [15] 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 #&gt; [29] 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 #&gt; [43] 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] obj.bigsnp$genotypes &lt;- G2 To always use G2 (with the new code256) and the extended obj.bigsnp$fam, you need to re-save obj.bigsnp using snp_save(obj.bigsnp) You can re-attach this data in another R session later using snp_attach(\"tmp-data/GWAS_data_sorted_QC.rds\"). "],["population-structure.html", "Chapter 4 Population structure 4.1 Principal Component Analysis (PCA) 4.2 Exercise: population structure", " Chapter 4 Population structure 4.1 Principal Component Analysis (PCA) PCA on the genotype matrix can be used to capture population structure. PCA can capture different kinds of structure (Privé, Luu, Blum, et al. 2020): population structure (what we want) LD structure, when there are two many correlated variants and not enough population structure (see this vignette) relatedness structure, when there are related individuals that can cluster together in later PCs noise, basically just circles when looking at PC scores In Privé et al. (2018), I have introduced an algorithm to compute PCA for a bigSNP object while accounting for the LD problem via an automatic removing of long-range LD regions. In Privé, Luu, Blum, et al. (2020), I have extended the package to be also able to run PCA directly on PLINK bed files with a few missing values, and investigated best practices for PCA in more detail. In Privé et al. (2021), I have shown how to use PCA for ancestry inference. There are many steps to compute properly a PCA analysis, which I will try to detail in the following exercise. 4.2 Exercise: population structure You can find another example in this vignette. Let us reuse the data prepared in 3.3. First, lets get an idea of the relatedness in the data using library(bigsnpr) plink2 &lt;- download_plink2(&quot;tmp-data&quot;) rel &lt;- snp_plinkKINGQC(plink2, &quot;tmp-data/GWAS_data_sorted_QC.bed&quot;, thr.king = 2^-4.5, make.bed = FALSE, ncores = nb_cores()) hist(log2(rel$KINSHIP), &quot;FD&quot;) When computing relatedness with KING, it may be needed to filter out some variants that are highly associated with population structure, e.g. as performed in the UK Biobank (Bycroft et al. 2018). [TODO: also export my code for iPSYCH] obj.bed &lt;- bed(&quot;tmp-data/GWAS_data_sorted_QC.bed&quot;) obj.svd &lt;- runonce::save_run( bed_autoSVD(obj.bed, k = 20, ncores = nb_cores()), file = &quot;tmp-data/PCA_GWAS_data.rds&quot;) #&gt; user system elapsed #&gt; 19.50 1.03 158.55 plot(obj.svd) plot(obj.svd, type = &quot;scores&quot;, scores = 1:8, coeff = 0.6) There is a bit of population structure (it seems at least 6 PCs). It would be better if could get a better sense of the ancestry of these individuals; let us project this data to the 1000G data and infer ancestry as described in Privé et al. (2021). bed.ref &lt;- bed(download_1000G(&quot;tmp-data&quot;)) proj &lt;- runonce::save_run( bed_projectPCA(bed.ref, obj.bed, k = 25, ncores = nb_cores()), file = &quot;tmp-data/proj-to-1000G.rds&quot;) #&gt; user system elapsed #&gt; 43.82 1.42 205.60 PC.ref &lt;- predict(proj$obj.svd.ref) proj2 &lt;- proj$OADP_proj fam2 &lt;- bigreadr::fread2(sub_bed(bed.ref$bedfile, &quot;.fam2&quot;)) library(ggplot2) source(&quot;https://raw.githubusercontent.com/privefl/paper4-bedpca/master/code/plot_grid2.R&quot;) plot_grid2(plotlist = lapply(1:9, function(k) { k1 &lt;- 2 * k - 1 k2 &lt;- 2 * k qplot(PC.ref[, k1], PC.ref[, k2], color = fam2$`Super Population`, size = I(2)) + geom_point(aes(proj2[, k1], proj2[, k2]), color = &quot;black&quot;, alpha = 0.1) + theme_bigstatsr(0.5) + labs(x = paste0(&quot;PC&quot;, k1), y = paste0(&quot;PC&quot;, k2), color = &quot;Ref Pop&quot;) + coord_equal() }), nrow = 3) These are clearly mostly European individuals. ldist &lt;- log(bigutilsr::dist_ogk(proj2)) lims &lt;- bigutilsr::hist_out(ldist)$lim hist(ldist, &quot;FD&quot;); abline(v = lims, col = &quot;red&quot;) sum(ldist &gt; lims[2]) #&gt; [1] 4 There are 4 outlier individuals that you could remove. "],["genome-wide-association-study-gwas.html", "Chapter 5 Genome-Wide Association Study (GWAS) 5.1 Exercise: GWAS", " Chapter 5 Genome-Wide Association Study (GWAS) In {bigstatsr}, you can perform both standard linear and logistic regressions GWAS, using either big_univLinReg() or big_univLogReg(). Function big_univLinReg() should be very fast. This type of association, where each variable is considered independently, can be performed for any type of FBM (i.e. it does not have to be a genotype matrix). This is why these two functions are in package {bigstatsr}, and not {bigsnpr}. 5.1 Exercise: GWAS Let us reuse the data prepared in 3.3 and in 4.2. library(bigsnpr) #&gt; Loading required package: bigstatsr obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes PC &lt;- predict(readRDS(&quot;tmp-data/PCA_GWAS_data.rds&quot;)) The clinical data includes age, sex, high-density lipoprotein (HDL)-cholesterol (hdl), low-density lipoprotein (LDL)-cholesterol (ldl), triglycerides (tg) and coronary artery disease status (CAD). For the set of covariates, we will use sex, age, and the first 6 PCs: covar &lt;- cbind(as.matrix(obj.bigsnp$fam[c(&quot;sex&quot;, &quot;age&quot;)]), PC[, 1:6]) Let us perform a case-control GWAS for CAD: y &lt;- obj.bigsnp$fam$CAD ind.gwas &lt;- which(!is.na(y) &amp; complete.cases(covar)) gwas &lt;- runonce::save_run( big_univLogReg(G, y[ind.gwas], ind.train = ind.gwas, covar.train = covar[ind.gwas, ], ncores = nb_cores()), file = &quot;tmp-data/GWAS_CAD.rds&quot;) #&gt; user system elapsed #&gt; 0.14 0.04 135.81 This takes two minutes with 4 cores on my laptop. Note that big_univLinReg() takes one second, and should give very similar p-values, if you just need something quick. You probably should not account for other information such as cholesterol as it is some heritable covariates (Aschard et al. 2015). plot(gwas) CHR &lt;- obj.bigsnp$map$chromosome POS &lt;- obj.bigsnp$map$physical.pos snp_manhattan(gwas, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) Here, nothing is genome-wide significant because of the small sample size. You can compare peaks with a GWAS for CAD with much larger sample size at https://pheweb.org/UKB-SAIGE/pheno/411.4. y2 &lt;- obj.bigsnp$fam$hdl ind.gwas2 &lt;- which(!is.na(y2) &amp; complete.cases(covar)) gwas2 &lt;- big_univLinReg(G, y2[ind.gwas2], ind.train = ind.gwas2, covar.train = covar[ind.gwas2, ], ncores = nb_cores()) snp_manhattan(gwas2, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) "],["polygenic-scores-pgs.html", "Chapter 6 Polygenic scores (PGS) 6.1 Exercises: LDpred2 and lassosum2", " Chapter 6 Polygenic scores (PGS) Polygenic scores are one of the main focus of package {bigsnpr}. These are the main methods currently available: penalized regressions, with individual-level data (Privé, Aschard, and Blum 2019) [tutorial] Clumping and Thresholding (C+T) and Stacked C+T (SCT), with summary statistics and individual level data (Privé et al. 2019) [tutorial] LDpred2, with summary statistics (Privé, Arbel, and Vilhjálmsson 2020) [tutorial] lassosum2, with the same input data as LDpred2 6.1 Exercises: LDpred2 and lassosum2 You should also check the other tutorials mentioned before. 6.1.1 Preparing the data Let us first read the data produced in 3.3: library(bigsnpr) obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes NCORES &lt;- nb_cores() map &lt;- dplyr::transmute(obj.bigsnp$map, chr = chromosome, pos = physical.pos, a0 = allele2, a1 = allele1) We then download some GWAS summary statistics from a large consortium (Nikpay et al. 2015; Buniello et al. 2019) and prepare them in the format required by LDpred2: txt &lt;- runonce::download_file( &quot;http://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST003001-GCST004000/GCST003116/cad.add.160614.website.txt&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD.txt&quot;) writeLines(readLines(txt, n = 3)) #&gt; markername chr bp_hg19 effect_allele noneffect_allele effect_allele_freq median_info model beta se_dgc p_dgc het_pvalue n_studies #&gt; rs143225517 1 751756 C T .158264 .92 FIXED .013006 .017324 .4528019 .303481 35 #&gt; rs3094315 1 752566 A G .763018 1 FIXED -.005243 .0157652 .7394597 .146867 36 # One could also read other variables such as &#39;median_info&#39;, &#39;model&#39;, # &#39;het_pvalue&#39;, and &#39;n_studies&#39; to apply some quality control to the sumstats. sumstats &lt;- bigreadr::fread2( txt, select = c(&quot;chr&quot;, &quot;bp_hg19&quot;, &quot;noneffect_allele&quot;, &quot;effect_allele&quot;, &quot;beta&quot;, &quot;se_dgc&quot;), col.names = c(&quot;chr&quot;, &quot;pos&quot;, &quot;a0&quot;, &quot;a1&quot;, &quot;beta&quot;, &quot;beta_se&quot;)) # GWAS effective sample size from the paper; # it would be better to have a per-variant sample size in the sumstats. # For quantitative traits, just use the total sample size for `n_eff`. sumstats$n_eff &lt;- 4 / (1 / 60801 + 1 / 123504) Let us now match these with the internal data we have: (info_snp &lt;- tibble::as_tibble(snp_match(sumstats, map))) #&gt; 9,455,778 variants to be matched. #&gt; 62,531 ambiguous SNPs have been removed. #&gt; 340,563 variants have been matched; 170,352 were flipped and 309,568 were reversed. #&gt; # A tibble: 340,563 x 9 #&gt; chr pos a0 a1 beta beta_se n_eff `_NUM_ID_.ss` #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 752566 T C 0.00524 0.0158 162973. 2 #&gt; 2 1 785989 G A -0.000981 0.0163 162973. 54 #&gt; 3 1 798959 G A 0.0156 0.0143 162973. 80 #&gt; 4 1 949608 G A -0.0401 0.0134 162973. 126 #&gt; 5 1 1018704 G A -0.0150 0.0101 162973. 246 #&gt; 6 1 1041700 T C -0.00894 0.0130 162973. 309 #&gt; 7 1 1129672 C A 0.00763 0.0147 162973. 584 #&gt; 8 1 1130727 A C 0.00143 0.0164 162973. 594 #&gt; 9 1 1158631 C T -0.0221 0.0208 162973. 719 #&gt; 10 1 1165310 C T 0.0278 0.0207 162973. 776 #&gt; # ... with 340,553 more rows, and 1 more variable: _NUM_ID_ &lt;int&gt; Note that we recommend to use imputed HapMap3 variants when available, otherwise you can just use the genotyped variants as I am doing here. Try to use an LD reference with at least 2000 individuals (I have only 1401 in this example). You can download some precomputed LD reference for European individuals based on the UK Biobank (+ some example R script and some information on the HapMap3 variants) at https://figshare.com/articles/dataset/European_LD_reference/13034123. Then we can perform some quality control on the summary statistics to see how standard deviations inferred from the external GWAS summary statistics are consistent with the ones in the internal data we have: maf &lt;- snp_MAF(G, ind.col = info_snp$`_NUM_ID_`, ncores = NCORES) sd_val &lt;- sqrt(2 * maf * (1 - maf)) sd_ss &lt;- with(info_snp, 2 / sqrt(n_eff * beta_se^2 + beta^2)) is_bad &lt;- sd_ss &lt; (0.5 * sd_val) | sd_ss &gt; (sd_val + 0.1) | sd_ss &lt; 0.1 | sd_val &lt; 0.05 When using quantitative traits (linear regression instead of logistic regression for the GWAS), you need to replace 2 by sd(y) when computing sd_ss. Have a look at section 3.4 of the LDpred2 paper. library(ggplot2) ggplot(dplyr::slice_sample(data.frame(sd_val, sd_ss, is_bad), n = 50e3)) + geom_point(aes(sd_val, sd_ss, color = is_bad), alpha = 0.5) + theme_bigstatsr(0.9) + scale_color_viridis_d(direction = -1) + geom_abline(linetype = 2) + labs(x = &quot;Standard deviations in the validation set&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;, color = &quot;To remove?&quot;, title = &quot;This is far from perfect; Neff is probably overestimated.&quot;) df_beta &lt;- info_snp[!is_bad, ] Then, we compute the correlation for each chromosome (note that we are using only 4 chromosomes for faster running of this tutorial): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- runonce::save_run({ ## indices in &#39;sumstats&#39; ind.chr &lt;- which(df_beta$chr == chr) ## indices in &#39;G&#39; ind.chr2 &lt;- df_beta$`_NUM_ID_`[ind.chr] POS2 &lt;- snp_asGeneticPos(map$chr[ind.chr2], map$pos[ind.chr2], dir = &quot;tmp-data&quot;) snp_cor(G, ind.col = ind.chr2, size = 3 / 1000, infos.pos = POS2, ncores = NCORES) }, file = paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) } #&gt; [1] 1 #&gt; user system elapsed #&gt; 61.94 1.11 39.75 #&gt; [1] 2 #&gt; user system elapsed #&gt; 78.23 1.39 55.51 #&gt; [1] 3 #&gt; user system elapsed #&gt; 79.24 1.04 52.75 #&gt; [1] 4 #&gt; user system elapsed #&gt; 53.17 0.74 35.75 Then we create the on-disk sparse genome-wide correlation matrix (again using only the first 4 chromosomes, for speed in this tutorial; replace by 1:22): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- readRDS(paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) if (chr == 1) { ld &lt;- Matrix::colSums(corr0^2) corr &lt;- as_SFBM(corr0, &quot;tmp-data/corr&quot;, compact = TRUE) } else { ld &lt;- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 Note that the compact format for SFBMs is quite new. You will need packageVersion(\"bigsparser\") &gt;= package_version(\"0.5\"). Make sure to reinstall {bigsnpr} when updating {bigsparser} to this new version (to avoid crashes). file.size(corr$sbk) / 1024^3 # file size in GB #&gt; [1] 0.5774516 Note that you will need at least the same memory as this file size (to keep it cached for faster processing) + some other memory for all the results returned. If you do not have enough memory, processing will be very slow (because you would read the data from disk all the time). If using HapMap3 variants, requesting 60 GB should be enough. For this small example, 8 GB of RAM should be enough. 6.1.2 LDpred2 We can now run LD score regression: df_beta &lt;- dplyr::filter(df_beta, chr %in% 1:4) # TO REMOVE (for speed here) (ldsc &lt;- with(df_beta, snp_ldsc(ld, length(ld), chi2 = (beta / beta_se)^2, sample_size = n_eff, blocks = NULL))) #&gt; int h2 #&gt; 0.87242475 0.02097793 ldsc_h2_est &lt;- ldsc[[&quot;h2&quot;]] We can now run LDpred2-inf very easily: # LDpred2-inf beta_inf &lt;- snp_ldpred2_inf(corr, df_beta, ldsc_h2_est) pred_inf &lt;- big_prodVec(G, beta_inf, ind.col = df_beta$`_NUM_ID_`) AUCBoot(pred_inf, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.64446627 0.61331759 0.67480551 0.01567758 For LDpred2(-grid), this is the grid we recommend to use: # LDpred2-grid (h2_seq &lt;- round(ldsc_h2_est * c(0.3, 0.7, 1, 1.4), 4)) #&gt; [1] 0.0063 0.0147 0.0210 0.0294 (p_seq &lt;- signif(seq_log(1e-5, 1, length.out = 21), 2)) #&gt; [1] 1.0e-05 1.8e-05 3.2e-05 5.6e-05 1.0e-04 1.8e-04 3.2e-04 5.6e-04 #&gt; [9] 1.0e-03 1.8e-03 3.2e-03 5.6e-03 1.0e-02 1.8e-02 3.2e-02 5.6e-02 #&gt; [17] 1.0e-01 1.8e-01 3.2e-01 5.6e-01 1.0e+00 params &lt;- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) dim(params) #&gt; [1] 168 3 Here, we will be using this smaller grid instead (for speed in this tutorial): (params &lt;- expand.grid(p = signif(seq_log(0.001, 0.3, length.out = 8), 2), h2 = round(ldsc_h2_est, 4), sparse = TRUE)) #&gt; p h2 sparse #&gt; 1 0.0010 0.021 TRUE #&gt; 2 0.0023 0.021 TRUE #&gt; 3 0.0051 0.021 TRUE #&gt; 4 0.0120 0.021 TRUE #&gt; 5 0.0260 0.021 TRUE #&gt; 6 0.0590 0.021 TRUE #&gt; 7 0.1300 0.021 TRUE #&gt; 8 0.3000 0.021 TRUE beta_grid &lt;- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES) params$sparsity &lt;- colMeans(beta_grid == 0) Then, we can compute the corresponding PGS for all these models: pred_grid &lt;- big_prodMat(G, beta_grid, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params$score &lt;- apply(pred_grid, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) Note that missing values represent models that diverged substantially. ggplot(params, aes(x = p, y = score, color = as.factor(h2))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0), minor_breaks = params$p) + facet_wrap(~ sparse, labeller = label_both) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;h2&quot;) + theme(legend.position = &quot;top&quot;, panel.spacing = unit(1, &quot;lines&quot;)) Then you can use the best-performing model here. Note that you should use only individuals from the validation set to compute the $score and then evaluate the best model for the individuals in the test set. library(dplyr) #&gt; Warning: package &#39;dplyr&#39; was built under R version 3.6.3 best_beta_grid &lt;- params %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_grid[, .] To run LDpred2-auto, you can use: # LDpred2-auto # Note: use vec_p_init = seq_log(1e-4, 0.5, 30) in real scenario # and defaults for &#39;burn_in&#39; and &#39;num_iter&#39; (again, for speed here) multi_auto &lt;- snp_ldpred2_auto( corr, df_beta, h2_init = ldsc_h2_est, vec_p_init = seq_log(1e-3, 0.1, NCORES), # TO CHANGE burn_in = 50, num_iter = 50, # TO REMOVE ncores = NCORES) We should perform some quality control on the chains (you should have more than 4, e.g. 30): beta_auto &lt;- sapply(multi_auto, function(auto) auto$beta_est) pred_auto &lt;- big_prodMat(G, beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) sc &lt;- apply(pred_auto, 2, sd) keep &lt;- abs(sc - median(sc)) &lt; 3 * mad(sc) final_beta_auto &lt;- rowMeans(beta_auto[, keep]) We can finally test the final prediction # could directly use `rowMeans(pred_auto[, keep])` here final_pred_auto &lt;- big_prodVec(G, final_beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) AUCBoot(final_pred_auto, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.60643211 0.57532254 0.63761716 0.01581748 6.1.3 lassosum2: grid of models lassosum2 is a re-implementation of the lassosum model that now uses the exact same input parameters as LDpred2 (corr and df_beta). It should be fast to run. It can be run next to LDpred2 and the best model can be chosen using the validation set. Note that parameter s from lassosum has been replaced by a new parameter delta in lassosum2, in order to better reflect that the lassosum model also uses L2-regularization (therefore, elastic-net regularization). beta_lassosum2 &lt;- snp_lassosum2( corr, df_beta, ncores = NCORES, nlambda = 10, maxiter = 50) # TO REMOVE params2 &lt;- attr(beta_lassosum2, &quot;grid_param&quot;) pred_grid2 &lt;- big_prodMat(G, beta_lassosum2, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params2$score &lt;- apply(pred_grid2, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) library(ggplot2) ggplot(params2, aes(x = lambda, y = score, color = as.factor(delta))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0)) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;delta&quot;) library(dplyr) best_grid_lassosum2 &lt;- params2 %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_lassosum2[, .] best_grid_overall &lt;- `if`(max(params2$score) &gt; max(params$score), best_grid_lassosum2, best_beta_grid) "],["references.html", "References", " References Aschard, Hugues, Bjarni J Vilhjálmsson, Amit D Joshi, Alkes L Price, and Peter Kraft. 2015. Adjusting for Heritable Covariates Can Bias Effect Estimates in Genome-Wide Association Studies. The American Journal of Human Genetics 96 (2): 32939. Buniello, Annalisa, Jacqueline A L MacArthur, Maria Cerezo, Laura W Harris, James Hayhurst, Cinzia Malangone, Aoife McMahon, et al. 2019. The NHGRI-EBI GWAS Catalog of Published Genome-Wide Association Studies, Targeted Arrays and Summary Statistics 2019. Nucleic Acids Research 47 (D1): D100512. Bycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T Elliott, Kevin Sharp, Allan Motyer, et al. 2018. The UK Biobank Resource with Deep Phenotyping and Genomic Data. Nature 562 (7726): 2039. Nikpay, Majid, Anuj Goel, Hong-Hee Won, Leanne M Hall, Christina Willenborg, Stavroula Kanoni, Danish Saleheen, et al. 2015. A Comprehensive 1000 GenomesBased Genome-Wide Association Meta-Analysis of Coronary Artery Disease. Nature Genetics 47 (10): 1121. Privé, Florian. 2021. Optimal Linkage Disequilibrium Splitting. bioRxiv. Privé, Florian, Julyan Arbel, and Bjarni J Vilhjálmsson. 2020. LDpred2: better, faster, stronger. Bioinformatics, December. https://doi.org/10.1093/bioinformatics/btaa1029. Privé, Florian, Hugues Aschard, and Michael GB Blum. 2019. Efficient Implementation of Penalized Regression for Genetic Risk Prediction. Genetics 212 (1): 6574. Privé, Florian, Hugues Aschard, Shai Carmi, Lasse Folkersen, Clive Hoggart, Paul F OReilly, and Bjarni J Vilhjálmsson. 2021. High-Resolution Portability of 245 Polygenic Scores When Derived and Applied in the Same Cohort. medRxiv. Privé, Florian, Hugues Aschard, Andrey Ziyatdinov, and Michael G B Blum. 2018. Efficient Analysis of Large-Scale Genome-Wide Data with Two R Packages: Bigstatsr and Bigsnpr. Bioinformatics 34 (16): 278187. https://doi.org/10.1093/bioinformatics/bty185. Privé, Florian, Keurcien Luu, Michael G B Blum, John J McGrath, and Bjarni J Vilhjálmsson. 2020. Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, May. https://doi.org/10.1093/bioinformatics/btaa520. Privé, Florian, Keurcien Luu, Bjarni J Vilhjálmsson, and Michael GB Blum. 2020. Performing Highly Efficient Genome Scans for Local Adaptation with R Package Pcadapt Version 4. Molecular Biology and Evolution 37 (7): 215354. Privé, Florian, Bjarni J Vilhjálmsson, Hugues Aschard, and Michael G. B. Blum. 2019. Making the Most of Clumping and Thresholding for Polygenic Scores. The American Journal of Human Genetics 105 (6): 121321. Reed, Eric, Sara Nunez, David Kulp, Jing Qian, Muredach P Reilly, and Andrea S Foulkes. 2015. A Guide to Genome-Wide Association Analysis and Post-Analytic Interrogation. Statistics in Medicine 34 (28): 376992. Visscher, Peter M, Naomi R Wray, Qian Zhang, Pamela Sklar, Mark I McCarthy, Matthew A Brown, and Jian Yang. 2017. 10 Years of GWAS Discovery: Biology, Function, and Translation. The American Journal of Human Genetics 101 (1): 522. "]]
