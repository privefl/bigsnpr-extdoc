[["index.html", "bigsnpr &amp; bigstatsr Extended documentation About Prerequisites License Author Contact", " bigsnpr &amp; bigstatsr Extended documentation Florian Privé 2022-11-02 About Prerequisites Have at least a basic knowledge of R, Install recent versions of R (&gt;= 3.4) and RStudio (&gt;= 1.2), Install recent versions of {bigstatsr} and {bigsnpr} (e.g. from CRAN), Download the data used in the tutorials: runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019072&quot;, dir = &quot;tmp-data&quot;, fname = &quot;GWAS_data.zip&quot;) # 109 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019027&quot;, dir = &quot;tmp-data&quot;, fname = &quot;ref_freqs.csv.gz&quot;) # 46 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019024&quot;, dir = &quot;tmp-data&quot;, fname = &quot;projection.csv.gz&quot;) # 44 MB runonce::download_file( &quot;https://figshare.com/ndownloader/files/38077323&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD_tuto.csv.gz&quot;) # 16 MB bigsnpr::download_plink(&quot;tmp-data&quot;) # 6.3 MB bigsnpr::download_plink2(&quot;tmp-data&quot;) # 6.6 MB #&gt; [1] &quot;tmp-data/GWAS_data.zip&quot; #&gt; [1] &quot;tmp-data/ref_freqs.csv.gz&quot; #&gt; [1] &quot;tmp-data/projection.csv.gz&quot; #&gt; [1] &quot;tmp-data/sumstats_CAD_tuto.csv.gz&quot; #&gt; [1] &quot;tmp-data/plink.exe&quot; #&gt; [1] &quot;tmp-data/plink2.exe&quot; License This material is licensed under the Creative Commons Attribution-ShareAlike 3.0 License. Author Florian Privé is a senior researcher in predictive human genetics, fond of Data Science and an R(cpp) enthusiast. You can find him on Twitter and GitHub as @privefl. Contact If you want me to add or clarify some content in this documentation, please open an issue on the GitHub repository of this documentation. If you have bug reports or questions specifically on functions of the packages, please open an issue on the corresponding packages repository. I will always redirect you to GitHub issues if you email me about the packages, so that others can benefit from our discussion. "],["introduction.html", "Chapter 1 Introduction 1.1 Main motivation for developing {bigstatsr} and {bigsnpr} 1.2 Features 1.3 Example code 1.4 Installation 1.5 Correct spellings", " Chapter 1 Introduction 1.1 Main motivation for developing {bigstatsr} and {bigsnpr} The main motivation was for me to be able to run all my analyses within . I was frustrated by having to use all these different software, with different input formats, and requiring text files for parameters. This made it hard for me to build a chain of analyses, to perform some exploratory analyses, or to use familiar packages. Also, I wanted to develop new methods, which seemed very hard to do without using a simple matrix-like format. Thus I started developing package {bigsnpr} at the beginning of my thesis. At some point, I realized that many functions (to perform e.g. GWAS, PCA, summary statistics) were not really specific to genotype data. Indeed, a TWAS or an EWAS are not conceptually very different from a GWAS; one can also perform PCA on e.g. DNA methylation data. Therefore I decided to move all these functions that could be used on any data stored as a matrix into a new package, {bigstatsr}. This is why there are two packages, where {bigstatsr} can basically be used by any field using data stored as large matrices, while {bigsnpr} provides some tools rather specific to genotype data, largely building on top of {bigstatsr}. The initial description of the two packages is available in Privé, Aschard, Ziyatdinov, &amp; Blum (2018). Functions starting with big_ are part of {bigstatsr}, while functions starting with snp_ or bed_ are part of {bigsnpr}. 1.2 Features There are now many things implemented in the packages. You can find a comprehensive list of available functions on the package website of {bigstatsr} and of {bigsnpr}. The next table presents an overview of common genetic analyses that are already implemented in {bigstatsr} and {bigsnpr}. This listing is inspired from table 1 of Visscher et al. (2017). Analysis Available in {bigstatsr} and {bigsnpr} Still missing Citations Polygenic risk scores - penalized regressions - (stacked) C+T - LDpred2 - lassosum2 multi-ancestry training Privé, Aschard, &amp; Blum (2019) Privé, Vilhjálmsson, Aschard, &amp; Blum (2019) Privé, Arbel, &amp; Vilhjálmsson (2020) Privé, Arbel, Aschard, &amp; Vilhjálmsson (2022) Population structure - principal component analysis - ancestry inference - fixation index (\\(F_{ST}\\)) - local adaptation Privé, Aschard, Ziyatdinov, &amp; Blum (2018) Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson (2020) Privé et al. (2022) Privé (2022) Privé, Luu, Vilhjálmsson, &amp; Blum (2020) GWAS linear and logistic - mixed models - rare variant association Privé, Aschard, Ziyatdinov, &amp; Blum (2018) Genome-wide assessment of LD - sparse correlation matrix - optimal LD splitting Privé, Aschard, Ziyatdinov, &amp; Blum (2018) Privé (2021) Estimation of SNP heritability - LD score regression - LDpred2-auto Privé, Arbel, &amp; Vilhjálmsson (2020) Privé, Albiñana, Pasaniuc, &amp; Vilhjálmsson (2022) Estimation of polygenicity LDpred2-auto Privé, Albiñana, Pasaniuc, &amp; Vilhjálmsson (2022) Estimation of genetic correlation need to extend LDpred2-auto Fine-mapping LDpred2-auto Privé, Albiñana, Pasaniuc, &amp; Vilhjálmsson (2022) GWAS summary imputation need to extend LDpred2-auto Mendelian randomization completely missing Miscellaneous - integration with PLINK - format conversion - imputation of genotyped variants - matrix operations - summaries Privé, Aschard, Ziyatdinov, &amp; Blum (2018) 1.3 Example code When you want to use a function for the first time, check the documentation and the examples in there (usually they are very short). There are also many (longer) tutorials available (usually one with each paper), which be linked from here or are available at the packages websites. Some other examples are provided in this extended documentation (in the next chapters). All the code used in all my papers is available on GitHub. It mostly consists of R scripts based on {bigsnpr}, {bigstatsr}, the tidyverse, and the futureverse (Bengtsson, 2021; Privé, Aschard, Ziyatdinov, &amp; Blum, 2018; Wickham et al., 2019). 1.4 Installation Both packages are available on CRAN, so you can use install.packages(): install.packages(&quot;bigstatsr&quot;) install.packages(&quot;bigsnpr&quot;) To install the latest versions (from GitHub), you can use {remotes}: # install.packages(&quot;remotes&quot;) remotes::install_github(&quot;privefl/bigstatsr&quot;) remotes::install_github(&quot;privefl/bigsnpr&quot;) 1.5 Correct spellings A friendly reminder:The correct spelling is - bigstatsr  not bigstatr / BIGstatsR - bigsnpr  not BIGsnpR / bigSNPr - pcadapt  not PCAdapt - LDpred  not LDPredThank youThe kittens thank you too pic.twitter.com/S8wyE4G6BG  Florian Privé ((privefl?)) November 18, 2020 References Bengtsson, H. (2021). A unifying framework for parallel and distributed processing in R using futures. The R Journal, 13, 273291. Privé, F. (2021). Optimal linkage disequilibrium splitting. Bioinformatics, 38, 255256. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 34773480. Privé, F., Albiñana, C., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2022). Inferring disease architecture and predictive ability with LDpred2-auto. bioRxiv. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 54245431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 6574. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., OReilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 1223. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 27812787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. Privé, F., Luu, K., Vilhjálmsson, B.J., &amp; Blum, M.G. (2020). Performing highly efficient genome scans for local adaptation with R package pcadapt version 4. Molecular Biology and Evolution, 37, 21532154. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 12131221. Visscher, P.M., Wray, N.R., Zhang, Q., Sklar, P., McCarthy, M.I., Brown, M.A., &amp; Yang, J. (2017). 10 years of GWAS discovery: Biology, function, and translation. The American Journal of Human Genetics, 101, 522. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R.,  others. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4, 1686. "],["inputs-and-formats.html", "Chapter 2 Inputs and formats 2.1 In {bigstatsr} 2.2 In {bigsnpr} 2.3 Getting an FBM or bigSNP object", " Chapter 2 Inputs and formats 2.1 In {bigstatsr} The format provided in package {bigstatsr} is called a Filebacked Big Matrix (FBM). It is an on-disk matrix format which is accessed through memory-mapping. How memory-mapping works: when you access the 1st element (1st row, 1st col), it accesses a block (say the first column) from disk into memory (RAM) when you access the 2nd element (2nd row, 1st col), it is already in memory so it is accessed very fast when you access the second column, you access from disk again (once) you can access many columns like that, until you do not have enough memory anymore some space is freed automatically so that new columns can be accessed into memory everything is seamlessly managed by the operating system (OS) it is also very convenient for parallelism as data is shared between processes All the elements of an FBM have the same type; supported types are: \"double\" (the default, double precision  64 bits) \"float\" (single precision  32 bits) \"integer\" (signed, so between \\(\\text{-}2^{31}\\) and \\(2^{31} \\text{ - } 1\\)) \"unsigned short\": can store integer values from \\(0\\) to \\(65535\\) \"raw\" or \"unsigned char\": can store integer values from \\(0\\) to \\(255\\). It is the basis for class FBM.code256 in order to access 256 arbitrary different numeric values. It is used in package {bigsnpr} (see below). 2.2 In {bigsnpr} Package {bigsnpr} uses a class called bigSNP for representing SNP data. A bigSNP object is merely a list with the following elements: $genotypes: A FBM.code256. Rows are samples and columns are genetic variants. This stores genotype calls or dosages (rounded to 2 decimal places). $fam: A data.frame with some information on the samples. $map: A data.frame with some information on the genetic variants. The code used in class FBM.code256 for imputed data is e.g. bigsnpr::CODE_DOSAGE #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 #&gt; [15] 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 #&gt; [29] 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 #&gt; [43] 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44 0.45 0.46 0.47 0.48 #&gt; [57] 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.60 0.61 0.62 #&gt; [71] 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74 0.75 0.76 #&gt; [85] 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.90 #&gt; [99] 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00 1.01 1.02 1.03 1.04 #&gt; [113] 1.05 1.06 1.07 1.08 1.09 1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 #&gt; [127] 1.19 1.20 1.21 1.22 1.23 1.24 1.25 1.26 1.27 1.28 1.29 1.30 1.31 1.32 #&gt; [141] 1.33 1.34 1.35 1.36 1.37 1.38 1.39 1.40 1.41 1.42 1.43 1.44 1.45 1.46 #&gt; [155] 1.47 1.48 1.49 1.50 1.51 1.52 1.53 1.54 1.55 1.56 1.57 1.58 1.59 1.60 #&gt; [169] 1.61 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 1.70 1.71 1.72 1.73 1.74 #&gt; [183] 1.75 1.76 1.77 1.78 1.79 1.80 1.81 1.82 1.83 1.84 1.85 1.86 1.87 1.88 #&gt; [197] 1.89 1.90 1.91 1.92 1.93 1.94 1.95 1.96 1.97 1.98 1.99 2.00 NA NA #&gt; [211] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [225] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [239] NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [253] NA NA NA NA where the first four elements are used to store genotype calls, the next three to store imputed allele counts, and the next 201 values to store dosages rounded to 2 decimal places. This allows for handling many types of data while storing each elements using one byte only (x4 compared to bed files, but /8 compared to doubles). Since v1.0, package {bigsnpr} also provides functions for directly working on bed files with a small percentage of missing values (Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson, 2020). If there is a demand for it, I might extend functions in {bigsnpr} to handle more types of FBMs than only FBM.code256. We have started talking about this in this issue. 2.3 Getting an FBM or bigSNP object The easiest way to get an FBM is to use the constructor function FBM() or the converter as_FBM(). To read an FBM from a large text file, you can use function big_read() (see this vignette). To read a bigSNP object from bed/bim/fam files, you can use functions snp_readBed() and snp_readBed2() (the second can read a subset of individuals/variants and use parallelism). To read BGEN files, you can use function snp_readBGEN(). This function takes around 40 minutes to read 1M variants for 400K individuals using 15 cores. Note that this function works only for BGEN v1.2 with probabilities stored as 8 bits (cf. this issue), which is the case for e.g. the UK Biobank files. To read any format used in genetics, you can always convert blocks of the data to text files using PLINK, read these using bigreadr::fread2(), and fill part of the resulting FBM. For example, see the code I used to convert the iPSYCH imputed data from the RICOPILI pipeline to my bigSNP format. References Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. "],["working-with-an-fbm.html", "Chapter 3 Working with an FBM 3.1 Similar accessor as R matrices 3.2 Split-(par)Apply-Combine Strategy 3.3 Similar accessor as Rcpp matrices 3.4 Some summary functions are already implemented", " Chapter 3 Working with an FBM 3.1 Similar accessor as R matrices library(bigstatsr) X &lt;- FBM(2, 5, init = 1:10, backingfile = &quot;test&quot;)$save() X$backingfile ## the file where the data is actually stored #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\Desktop\\\\bigsnpr-extdoc\\\\test.bk&quot; X &lt;- big_attach(&quot;test.rds&quot;) ## can get the FBM from any R session X[, 1] ## ok #&gt; [1] 1 2 X[1, ] ## bad #&gt; [1] 1 3 5 7 9 X[] ## super bad #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 1 3 5 7 9 #&gt; [2,] 2 4 6 8 10 You can access the whole FBM as an R matrix in memory using X[]. However, if the matrix is too large to fit in memory, you should always access only a subset of columns. Note that the elements of the FBM are stored column-wise (as for a standard R matrix). Therefore, be careful not to access a subset of rows, since it would read non-contiguous elements from the whole matrix from disk. 3.2 Split-(par)Apply-Combine Strategy colSums(X[]) ## super bad #&gt; [1] 3 7 11 15 19 How to apply standard R functions to big matrices (in parallel); implemented in big_apply(). Learn more with this tutorial on big_apply(). Exercise: compute the sum of each column using big_apply(). 3.3 Similar accessor as Rcpp matrices // [[Rcpp::plugins(cpp11)]] // [[Rcpp::depends(bigstatsr, rmio)]] #include &lt;bigstatsr/BMAcc.h&gt; // [[Rcpp::export]] NumericVector bigcolsums(Environment BM) { XPtr&lt;FBM&gt; xpBM = BM[&quot;address&quot;]; // get the external pointer BMAcc&lt;double&gt; macc(xpBM); // create an accessor to the data size_t n = macc.nrow(); // similar code as for an Rcpp::NumericMatrix size_t m = macc.ncol(); // similar code as for an Rcpp::NumericMatrix NumericVector res(m); for (size_t j = 0; j &lt; m; j++) for (size_t i = 0; i &lt; n; i++) res[j] += macc(i, j); // similar code as for an Rcpp::NumericMatrix return res; } For a subset of the data: // [[Rcpp::plugins(cpp11)]] // [[Rcpp::depends(bigstatsr, rmio)]] #include &lt;bigstatsr/BMAcc.h&gt; // [[Rcpp::export]] NumericVector bigcolsums2(Environment BM, const IntegerVector&amp; rowInd, const IntegerVector&amp; colInd) { XPtr&lt;FBM&gt; xpBM = BM[&quot;address&quot;]; // accessor to a sub-view of the data -&gt; the only line of code that should change SubBMAcc&lt;double&gt; macc(xpBM, rowInd, colInd, 1); size_t n = macc.nrow(); size_t m = macc.ncol(); NumericVector res(m); for (size_t j = 0; j &lt; m; j++) for (size_t i = 0; i &lt; n; i++) res[j] += macc(i, j); return res; } 3.4 Some summary functions are already implemented big_colstats(X) # sum and var (for each column) #&gt; sum var #&gt; 1 3 0.5 #&gt; 2 7 0.5 #&gt; 3 11 0.5 #&gt; 4 15 0.5 #&gt; 5 19 0.5 big_scale()(X) # mean and sd (for each column) #&gt; center scale #&gt; 1 1.5 0.7071068 #&gt; 2 3.5 0.7071068 #&gt; 3 5.5 0.7071068 #&gt; 4 7.5 0.7071068 #&gt; 5 9.5 0.7071068 To only use a subset of the data stored as an FBM, you should almost never make a copy of the data; instead, use parameters ind.row (or ind.train) and ind.col to apply functions to a subset of the data. "],["preprocessing.html", "Chapter 4 Preprocessing 4.1 Conversion and quality control of PLINK files 4.2 Imputation 4.3 Example", " Chapter 4 Preprocessing In this section, I talk about conversion, quality control and imputation. Conversion is also discussed in 2.3. 4.1 Conversion and quality control of PLINK files PLINK is very efficient for conversion and quality control of multiple formats, so I provide some wrappers to PLINK in {bigsnpr}, for ease of use directly from : download_plink() and download_plink2() for downloading the latest stable versions of PLINK 1.9 and 2.0 (Chang et al., 2015). snp_plinkQC() for quality control (QC) and conversion to bed/bim/fam. snp_plinkKINGQC() for QC on relatedness based on KING-robust kinship estimator (Manichaikul et al., 2010). Using make.bed = FALSE allows for computing related pairs only, i.e. reporting a data frame without producing new bed/bim/fam files. Note that monozygotic twins or identical samples have a KING coefficient of \\(0.5\\), not \\(1\\); \\(0.25\\) = siblings and parents; \\(2^{-3}\\) = second-degree relatives (e.g. grandparents, uncles); \\(2^{-4}\\) = third-degree relatives (e.g. cousins). You can use a threshold of \\(2^{-4.5} \\approx 0.0442\\) to remove all these related individuals (one from each pair). snp_plinkIBDQC() for QC based on identity-by-descent (IBD) computed by PLINK using its method-of-moments. I prefer the KING one. snp_plinkRmSamples() for producing new PLINK files after having removed some individuals. For any other PLINK function, I recommend calling PLINK directly from thanks to system calls and package {glue}, e.g. plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --version&quot; )) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) 4.2 Imputation Note that most functions from {bigstatsr} and {bigsnpr} do NOT handle missing values. Simple imputation (e.g. by the mean) of a double FBM can be performed by blocks using e.g. the code from this vignette. In {bigsnpr}, to perform simple imputation of genotyped data, you can use snp_fastImputeSimple(). You can also use the slower snp_fastImpute() that uses XGBoost models to impute genotyped data (Privé, Aschard, Ziyatdinov, &amp; Blum, 2018; Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson, 2020). 4.3 Example For the exercises, we will use the data provided in Reed et al. (2015). This can be downloaded using zip &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019072&quot;, dir = &quot;tmp-data&quot;, fname = &quot;GWAS_data.zip&quot;) unzip(zip, exdir = &quot;tmp-data&quot;, overwrite = FALSE) For some reason, this data is not ordered by chromosome and position; we can use PLINK to get an ordered version of this using library(bigsnpr) plink &lt;- download_plink(&quot;tmp-data&quot;) system(glue::glue( &quot;{plink} --bfile tmp-data/GWAS_data&quot;, &quot; --make-bed --out tmp-data/GWAS_data_sorted&quot; )) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) www.cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2022 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data #&gt; --make-bed #&gt; --out tmp-data/GWAS_data_sorted #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 500000 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted.bed + tmp-data/GWAS_data_sorted.bim + #&gt; tmp-data/GWAS_data_sorted.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. As you can see from PLINK output, this data contains 1401 individuals and 500,000 variants, with a few missing values. We can then perform some quality control using bedfile2 &lt;- snp_plinkQC(plink, &quot;tmp-data/GWAS_data_sorted&quot;) #&gt; PLINK v1.90b6.26 64-bit (2 Apr 2022) www.cog-genomics.org/plink/1.9/ #&gt; (C) 2005-2022 Shaun Purcell, Christopher Chang GNU General Public License v3 #&gt; Logging to tmp-data/GWAS_data_sorted_QC.log. #&gt; Options in effect: #&gt; --bfile tmp-data/GWAS_data_sorted #&gt; --geno 0.1 #&gt; --hwe 1e-50 #&gt; --maf 0.01 #&gt; --make-bed #&gt; --mind 0.1 #&gt; --out tmp-data/GWAS_data_sorted_QC #&gt; #&gt; 32574 MB RAM detected; reserving 16287 MB for main workspace. #&gt; 500000 variants loaded from .bim file. #&gt; 1401 people (937 males, 464 females) loaded from .fam. #&gt; 933 phenotype values loaded from .fam. #&gt; 0 people removed due to missing genotype data (--mind). #&gt; Using 1 thread (no multithreaded calculations invoked). #&gt; Before main variant filters, 1401 founders and 0 nonfounders present. #&gt; Calculating allele frequencies... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\b\b done. #&gt; Total genotyping rate is 0.977593. #&gt; 27390 variants removed due to missing genotype data (--geno). #&gt; --hwe: 91 variants removed due to Hardy-Weinberg exact test. #&gt; 67856 variants removed due to minor allele threshold(s) #&gt; (--maf/--max-maf/--mac/--max-mac). #&gt; 404663 variants and 1401 people pass filters and QC. #&gt; Among remaining phenotypes, 463 are cases and 470 are controls. (468 #&gt; phenotypes are missing.) #&gt; --make-bed to tmp-data/GWAS_data_sorted_QC.bed + #&gt; tmp-data/GWAS_data_sorted_QC.bim + tmp-data/GWAS_data_sorted_QC.fam ... 0%\b\b1%\b\b2%\b\b3%\b\b4%\b\b5%\b\b6%\b\b7%\b\b8%\b\b9%\b\b10%\b\b\b11%\b\b\b12%\b\b\b13%\b\b\b14%\b\b\b15%\b\b\b16%\b\b\b17%\b\b\b18%\b\b\b19%\b\b\b20%\b\b\b21%\b\b\b22%\b\b\b23%\b\b\b24%\b\b\b25%\b\b\b26%\b\b\b27%\b\b\b28%\b\b\b29%\b\b\b30%\b\b\b31%\b\b\b32%\b\b\b33%\b\b\b34%\b\b\b35%\b\b\b36%\b\b\b37%\b\b\b38%\b\b\b39%\b\b\b40%\b\b\b41%\b\b\b42%\b\b\b43%\b\b\b44%\b\b\b45%\b\b\b46%\b\b\b47%\b\b\b48%\b\b\b49%\b\b\b50%\b\b\b51%\b\b\b52%\b\b\b53%\b\b\b54%\b\b\b55%\b\b\b56%\b\b\b57%\b\b\b58%\b\b\b59%\b\b\b60%\b\b\b61%\b\b\b62%\b\b\b63%\b\b\b64%\b\b\b65%\b\b\b66%\b\b\b67%\b\b\b68%\b\b\b69%\b\b\b70%\b\b\b71%\b\b\b72%\b\b\b73%\b\b\b74%\b\b\b75%\b\b\b76%\b\b\b77%\b\b\b78%\b\b\b79%\b\b\b80%\b\b\b81%\b\b\b82%\b\b\b83%\b\b\b84%\b\b\b85%\b\b\b86%\b\b\b87%\b\b\b88%\b\b\b89%\b\b\b90%\b\b\b91%\b\b\b92%\b\b\b93%\b\b\b94%\b\b\b95%\b\b\b96%\b\b\b97%\b\b\b98%\b\b\b99%\b\b\bdone. 404,663 variants are remaining after this quality control; we can then read this data into an R object called bigSNP using (rds &lt;- snp_readBed2(bedfile2, ncores = nb_cores())) #&gt; [1] &quot;C:\\\\Users\\\\au639593\\\\Desktop\\\\bigsnpr-extdoc\\\\tmp-data\\\\GWAS_data_sorted_QC.rds&quot; obj.bigsnp &lt;- snp_attach(rds) str(obj.bigsnp, max.level = 2) #&gt; List of 3 #&gt; $ genotypes:Reference class &#39;FBM.code256&#39; [package &quot;bigstatsr&quot;] with 16 fields #&gt; ..and 26 methods, of which 12 are possibly relevant: #&gt; .. add_columns, as.FBM, bm, bm.desc, check_dimensions, #&gt; .. check_write_permissions, copy#envRefClass, initialize, #&gt; .. initialize#FBM, save, show#envRefClass, show#FBM #&gt; $ fam :&#39;data.frame&#39;: 1401 obs. of 6 variables: #&gt; ..$ family.ID : int [1:1401] 10002 10004 10005 10007 10008 10009 10010 10011 10012 10013 ... #&gt; ..$ sample.ID : int [1:1401] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ paternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ maternal.ID: int [1:1401] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ sex : int [1:1401] 1 2 1 1 1 1 1 2 1 2 ... #&gt; ..$ affection : int [1:1401] 1 1 2 1 2 2 2 1 2 -9 ... #&gt; $ map :&#39;data.frame&#39;: 404663 obs. of 6 variables: #&gt; ..$ chromosome : int [1:404663] 1 1 1 1 1 1 1 1 1 1 ... #&gt; ..$ marker.ID : chr [1:404663] &quot;rs12565286&quot; &quot;rs3094315&quot; &quot;rs2980319&quot; &quot;rs2980300&quot; ... #&gt; ..$ genetic.dist: int [1:404663] 0 0 0 0 0 0 0 0 0 0 ... #&gt; ..$ physical.pos: int [1:404663] 721290 752566 777122 785989 798959 947034 949608 1018704 1041700 1129672 ... #&gt; ..$ allele1 : chr [1:404663] &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; ... #&gt; ..$ allele2 : chr [1:404663] &quot;C&quot; &quot;T&quot; &quot;T&quot; &quot;G&quot; ... #&gt; - attr(*, &quot;class&quot;)= chr &quot;bigSNP&quot; We can store some extra information on the individuals (e.g. some phenotypes): clinical &lt;- bigreadr::fread2(&quot;tmp-data/GWAS_clinical.csv&quot;) # Get the same order as for the genotypes # (to match over multiple columns, use `vctrs::vec_match()`) ord &lt;- match(obj.bigsnp$fam$family.ID, clinical$FamID) pheno &lt;- clinical[ord, ] # Quick check stopifnot(all.equal(obj.bigsnp$fam$sex, pheno$sex)) # Update the $fam component obj.bigsnp$fam &lt;- cbind(obj.bigsnp$fam, pheno[-c(1, 3)]) Recall that this data contains some missing values; you can get some counts per variant using G &lt;- obj.bigsnp$genotypes counts &lt;- big_counts(G) counts[, 1:8] #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 hist(nbNA &lt;- counts[4, ]) We can e.g. perform a quick imputation by the mean using G2 &lt;- snp_fastImputeSimple(G, method = &quot;mean2&quot;, ncores = nb_cores()) big_counts(G2, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 0 0 0 0 0 0 0 0 #&gt; 0.01 0 0 0 0 0 0 0 0 #&gt; 0.02 0 0 0 0 0 0 0 0 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 196 rows ] big_counts(G, ind.col = 1:8) #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] #&gt; 0 1247 958 1057 988 831 1201 496 386 #&gt; 1 131 362 316 370 502 115 676 730 #&gt; 2 6 66 28 25 67 7 165 216 #&gt; &lt;NA&gt; 17 15 0 18 1 78 64 69 G still has missing values, but G2 does not. Note that both use the same underlying data (the same binary file on disk), the difference is that they use a different code to decode the underlying data: G$code256 #&gt; [1] 0 1 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [24] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA #&gt; [47] NA NA NA NA #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] G2$code256 #&gt; [1] 0.00 1.00 2.00 NA 0.00 1.00 2.00 0.00 0.01 0.02 0.03 0.04 0.05 0.06 #&gt; [15] 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18 0.19 0.20 #&gt; [29] 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.30 0.31 0.32 0.33 0.34 #&gt; [43] 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 #&gt; [ reached getOption(&quot;max.print&quot;) -- omitted 206 entries ] To always use G2 (with the new code256) and the extended obj.bigsnp$fam, you need to save obj.bigsnp again using obj.bigsnp$genotypes &lt;- G2 snp_save(obj.bigsnp) You can re-attach this data in another R session later using snp_attach(\"tmp-data/GWAS_data_sorted_QC.rds\"). References Chang, C.C., Chow, C.C., Tellier, L.C., Vattikuti, S., Purcell, S.M., &amp; Lee, J.J. (2015). Second-generation PLINK: Rising to the challenge of larger and richer datasets. Gigascience, 4, s13742015. Manichaikul, A., Mychaleckyj, J.C., Rich, S.S., Daly, K., Sale, M., &amp; Chen, W.-M. (2010). Robust relationship inference in genome-wide association studies. Bioinformatics, 26, 28672873. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 27812787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. Reed, E., Nunez, S., Kulp, D., Qian, J., Reilly, M.P., &amp; Foulkes, A.S. (2015). A guide to genome-wide association analysis and post-analytic interrogation. Statistics in Medicine, 34, 37693792. "],["population-structure.html", "Chapter 5 Population structure 5.1 Principal Component Analysis (PCA) 5.2 Example", " Chapter 5 Population structure 5.1 Principal Component Analysis (PCA) PCA on the genotype matrix can be used to capture population structure. PCA can capture different kinds of structure (Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson, 2020): population structure (what we want), LD structure, when there are two many correlated variants and not enough population structure (see this vignette), relatedness structure, when there are related individuals that can cluster together in later PCs, noise, basically just circles when looking at PC scores. Population structure is the second main topic of my research work (after polygenic scores). In Privé, Aschard, Ziyatdinov, &amp; Blum (2018), I introduced an algorithm to compute PCA for a bigSNP object while accounting for the LD problem by using clumping (not pruning, cf. this vignette) and an automatic removal of long-range LD regions. In Privé, Luu, Vilhjálmsson, &amp; Blum (2020), I improved an algorithm to detect variants associated with population structure. In Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson (2020), I extended the package to be also able to run PCA directly on PLINK bed files with a small percentage of missing values, and investigated best practices for PCA in more detail. In Privé et al. (2022) and Privé (2022), I showed how to use PCA for ancestry inference, including grouping individuals in homogeneous ancestry groups, and inferring ancestry proportions from allele frequencies only (see this vignette). 5.2 Example There are many steps to properly perform a PCA, which I will try to detail in the following exercise. You can find another example in this vignette. Let us reuse the data prepared in 4.3. First, let us get an idea of the relatedness in the data using library(bigsnpr) #&gt; Loading required package: bigstatsr (NCORES &lt;- nb_cores()) plink2 &lt;- download_plink2(&quot;tmp-data&quot;) rel &lt;- snp_plinkKINGQC(plink2, &quot;tmp-data/GWAS_data_sorted_QC.bed&quot;, thr.king = 2^-4.5, make.bed = FALSE, ncores = NCORES) hist(log2(rel$KINSHIP), &quot;FD&quot;) When computing relatedness with KING, it may be needed to filter out some variants that are highly associated with population structure, e.g. as performed in the UK Biobank (Bycroft et al., 2018). For example, see this code. Relatedness should not be a major issue here. Let us now compute PCs. obj.bed &lt;- bed(&quot;tmp-data/GWAS_data_sorted_QC.bed&quot;) obj.svd &lt;- runonce::save_run( bed_autoSVD(obj.bed, k = 12, ncores = NCORES), file = &quot;tmp-data/PCA_GWAS_data.rds&quot;) #&gt; user system elapsed #&gt; 19.51 1.51 145.78 plot(obj.svd) plot(obj.svd, type = &quot;scores&quot;, scores = 1:12, coeff = 0.5) There is some population structure (maybe up to 6 PCs). You should also check loadings to make sure there is no LD structure (peaks on loadings): plot(obj.svd, type = &quot;loadings&quot;, loadings = 1:8, coeff = 0.5) No peaks, but loadings of PC4 and PC5 are a bit odd. If you expect the individuals to mostly come from one population, you can simply use a robust distance to identify a homogeneous subset of individuals, then look at the histogram of log-distances to choose a threshold based on visual inspection (here I would probably choose 4.5). PC &lt;- predict(obj.svd) ldist &lt;- log(bigutilsr::dist_ogk(PC)) hist(ldist, &quot;FD&quot;) library(ggplot2) devtools::source_url( &quot;https://raw.githubusercontent.com/privefl/paper4-bedpca/master/code/plot_grid2.R&quot;) plot_grid2(plotlist = lapply(1:4, function(k) { k1 &lt;- 2 * k - 1 k2 &lt;- 2 * k qplot(PC[, k1], PC[, k2], color = ldist, size = I(2)) + scale_color_viridis_c() + theme_bigstatsr(0.6) + labs(x = paste0(&quot;PC&quot;, k1), y = paste0(&quot;PC&quot;, k2), color = &quot;log-distance&quot;) + coord_equal() }), nrow = 2, legend_ratio = 0.2, title_ratio = 0) It would be nice if we could get a better sense of the ancestry of these individuals. To achieve this, we can project this data onto the PCA space of many known population groups defined in Privé (2022). all_freq &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019027&quot;, # for the tutorial (46 MB) # &quot;https://figshare.com/ndownloader/files/31620968&quot;, # for real analyses (849 MB) dir = &quot;tmp-data&quot;, fname = &quot;ref_freqs.csv.gz&quot;)) projection &lt;- bigreadr::fread2( runonce::download_file( &quot;https://figshare.com/ndownloader/files/38019024&quot;, # for the tutorial (44 MB) # &quot;https://figshare.com/ndownloader/files/31620953&quot;, # for real analyses (847 MB) dir = &quot;tmp-data&quot;, fname = &quot;projection.csv.gz&quot;)) correction &lt;- c(1, 1, 1, 1.008, 1.021, 1.034, 1.052, 1.074, 1.099, 1.123, 1.15, 1.195, 1.256, 1.321, 1.382, 1.443) library(dplyr) matched &lt;- obj.bed$map %&gt;% transmute(chr = chromosome, pos = physical.pos, a1 = allele1, a0 = allele2) %&gt;% mutate(beta = 1) %&gt;% snp_match(all_freq[1:5]) %&gt;% print() #&gt; chr pos a0 a1 beta _NUM_ID_.ss rsid _NUM_ID_ #&gt; 1 1 752566 G A -1 2 rs3094315 1 #&gt; 2 1 785989 T C -1 4 rs2980300 2 #&gt; 3 1 798959 G A 1 5 rs11240777 3 #&gt; 4 1 947034 G A -1 6 rs2465126 4 #&gt; 5 1 949608 G A 1 7 rs1921 5 #&gt; 6 1 1018704 A G -1 8 rs9442372 6 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 301150 rows ] # further subsetting on missing values counts &lt;- bed_counts(obj.bed, ind.col = matched$`_NUM_ID_.ss`, ncores = NCORES) # hist(nb_na &lt;- counts[4, ]) ind &lt;- which(counts[4, ] &lt; (nrow(obj.bed) * 0.05)) # project individuals (divided by 2) onto the PC space PROJ &lt;- as.matrix(projection[matched$`_NUM_ID_`[ind], -(1:5)]) all_proj &lt;- apply(sweep(PROJ, 2, correction / 2, &#39;*&#39;), 2, function(x) { bed_prodVec(obj.bed, x, ind.col = matched$`_NUM_ID_.ss`[ind], ncores = NCORES, # scaling to get G if beta = 1 and (2 - G) if beta = -1 center = 1 - matched$beta[ind], scale = matched$beta[ind]) }) We can then assign individuals to the closest center: all_centers &lt;- crossprod(as.matrix(all_freq[matched$`_NUM_ID_`[ind], -(1:5)]), PROJ) all_sq_dist &lt;- apply(all_centers, 1, function(one_center) { rowSums(sweep(all_proj, 2, one_center, &#39;-&#39;)^2) }) THR &lt;- 0.005 # you can adjust this threshold thr_sq_dist &lt;- max(dist(all_centers)^2) * THR / 0.16 group &lt;- colnames(all_freq)[-(1:5)] group[group %in% c(&quot;Scandinavia&quot;, &quot;United Kingdom&quot;, &quot;Ireland&quot;)] &lt;- &quot;Europe (North West)&quot; group[group %in% c(&quot;Europe (South East)&quot;, &quot;Europe (North East)&quot;)] &lt;- &quot;Europe (East)&quot; cluster &lt;- group[ apply(all_sq_dist, 1, function(x) { ind &lt;- which.min(x) if (x[ind] &lt; thr_sq_dist) ind else NA }) ] table(cluster, exclude = NULL) # no NA -&gt; all assigned #&gt; cluster #&gt; Ashkenazi Europe (East) Europe (North West) #&gt; 111 148 873 #&gt; Europe (South West) Italy Middle East #&gt; 46 219 4 plot_grid2(plotlist = lapply(1:4, function(k) { k1 &lt;- 2 * k - 1 k2 &lt;- 2 * k qplot(PC[, k1], PC[, k2], color = cluster, size = I(2)) + theme_bigstatsr(0.6) + labs(x = paste0(&quot;PC&quot;, k1), y = paste0(&quot;PC&quot;, k2), color = &quot;Assigned\\npopulation&quot;) + coord_equal() }), nrow = 2, legend_ratio = 0.25, title_ratio = 0) These are mostly European individuals. PC4-PC6 are definitively a bit odd. References Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K.,  others. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203209. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 34773480. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., OReilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 1223. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 27812787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. Privé, F., Luu, K., Vilhjálmsson, B.J., &amp; Blum, M.G. (2020). Performing highly efficient genome scans for local adaptation with R package pcadapt version 4. Molecular Biology and Evolution, 37, 21532154. "],["genome-wide-association-study-gwas.html", "Chapter 6 Genome-Wide Association Study (GWAS) 6.1 Example", " Chapter 6 Genome-Wide Association Study (GWAS) In {bigstatsr}, you can perform both standard linear and logistic regressions GWAS, using either big_univLinReg() or big_univLogReg(). Function big_univLinReg() should be very fast, while big_univLogReg() is slower. This type of association, where each variable is considered independently, can be performed for any type of FBM (i.e. it does not have to be a genotype matrix). This is why these two functions are in package {bigstatsr}, and not {bigsnpr}. 6.1 Example Let us reuse the data prepared in 4.3 and the PCs in 5.2. library(bigsnpr) #&gt; Loading required package: bigstatsr obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) obj.svd &lt;- readRDS(&quot;tmp-data/PCA_GWAS_data.rds&quot;) PC &lt;- predict(obj.svd) The clinical data includes age, sex, high-density lipoprotein (HDL)-cholesterol (hdl), low-density lipoprotein (LDL)-cholesterol (ldl), triglycerides (tg) and coronary artery disease status (CAD). For the set of covariates, we will use sex, age, and the first 6 PCs: covar &lt;- cbind(as.matrix(obj.bigsnp$fam[c(&quot;sex&quot;, &quot;age&quot;)]), PC[, 1:6]) You probably should not account for other information such as cholesterol as they are heritable covariates (Aschard, Vilhjálmsson, Joshi, Price, &amp; Kraft, 2015). G &lt;- obj.bigsnp$genotypes y &lt;- obj.bigsnp$fam$CAD ind.gwas &lt;- which(!is.na(y) &amp; complete.cases(covar)) To only use a subset of the data stored as an FBM (G here), you should almost never make a copy of the data; instead, use parameters ind.row (or ind.train) and ind.col to apply functions to a subset of the data. Let us perform a case-control GWAS for CAD: gwas &lt;- runonce::save_run( big_univLogReg(G, y[ind.gwas], ind.train = ind.gwas, covar.train = covar[ind.gwas, ], ncores = nb_cores()), file = &quot;tmp-data/GWAS_CAD.rds&quot;) #&gt; user system elapsed #&gt; 0.66 0.43 123.44 This takes about two minutes with 4 cores on my laptop. Note that big_univLinReg() takes two seconds only, and should give very similar p-values, if you just need something quick. plot(gwas) CHR &lt;- obj.bigsnp$map$chromosome POS &lt;- obj.bigsnp$map$physical.pos snp_manhattan(gwas, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) Here, nothing is genome-wide significant because of the small sample size. y2 &lt;- obj.bigsnp$fam$hdl ind.gwas2 &lt;- which(!is.na(y2) &amp; complete.cases(covar)) gwas2 &lt;- big_univLinReg(G, y2[ind.gwas2], ind.train = ind.gwas2, covar.train = covar[ind.gwas2, ], ncores = nb_cores()) snp_manhattan(gwas2, CHR, POS, npoints = 50e3) + ggplot2::geom_hline(yintercept = -log10(5e-8), linetype = 2, color = &quot;red&quot;) Some other example code: GWAS in iPSYCH; you can perform the GWAS on multiple nodes in parallel that would each process a chunk of the variants only GWAS for very large data and multiple phenotypes; you should perform the GWAS for all phenotypes for a small chunk of columns to avoid repeated access from disk, and can process these chunks on multiple nodes in parallel some template for {future.batchtools} when using Slurm References Aschard, H., Vilhjálmsson, B.J., Joshi, A.D., Price, A.L., &amp; Kraft, P. (2015). Adjusting for heritable covariates can bias effect estimates in genome-wide association studies. The American Journal of Human Genetics, 96, 329339. "],["polygenic-scores-pgs.html", "Chapter 7 Polygenic scores (PGS) 7.1 Example: LDpred2 and lassosum2", " Chapter 7 Polygenic scores (PGS) PGS methods are the main topic of my research work. These are the main methods currently available in the packages: penalized regressions, with individual-level data (Privé, Aschard, &amp; Blum (2019) + tutorial) Clumping and Thresholding (C+T) and Stacked C+T (SCT), with summary statistics and individual level data (Privé, Vilhjálmsson, Aschard, &amp; Blum (2019) + tutorial) LDpred2, with summary statistics (Privé, Arbel, &amp; Vilhjálmsson (2020) + tutorial) lassosum2, with the same input data as LDpred2 (Privé, Arbel, Aschard, &amp; Vilhjálmsson (2022) + tutorial) You can now use LDpred2-auto for inference (Privé, Albiñana, Pasaniuc, &amp; Vilhjálmsson (2022) + tutorial). 7.1 Example: LDpred2 and lassosum2 You should also check the other tutorial mentioned before. 7.1.1 Preparing the data Let us first read the data produced in 4.3: library(bigsnpr) #&gt; Loading required package: bigstatsr obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes NCORES &lt;- nb_cores() map &lt;- dplyr::transmute(obj.bigsnp$map, chr = chromosome, pos = physical.pos, a0 = allele2, a1 = allele1) Download some GWAS summary statistics for CAD that I derived from the UK Biobank (Bycroft et al., 2018), and prepare them in the format required by LDpred2: gz &lt;- runonce::download_file( &quot;https://figshare.com/ndownloader/files/38077323&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD_tuto.csv.gz&quot;) readLines(gz, n = 3) #&gt; [1] &quot;chr,pos,rsid,allele1,allele2,freq,info,beta,se&quot; #&gt; [2] &quot;1,721290,rs12565286,C,G,0.035889027911808,0.941918079726998,0.0361758959140647,0.0290865883937757&quot; #&gt; [3] &quot;1,752566,rs3094315,A,G,0.840799909379283,0.997586884856296,-0.0340838522604864,0.0144572980122262&quot; sumstats &lt;- bigreadr::fread2( gz, select = c(&quot;chr&quot;, &quot;pos&quot;, &quot;allele2&quot;, &quot;allele1&quot;, &quot;beta&quot;, &quot;se&quot;, &quot;freq&quot;, &quot;info&quot;), col.names = c(&quot;chr&quot;, &quot;pos&quot;, &quot;a0&quot;, &quot;a1&quot;, &quot;beta&quot;, &quot;beta_se&quot;, &quot;freq&quot;, &quot;info&quot;)) # GWAS effective sample size for binary traits (4 / (1 / n_case + 1 / n_control)) # For quantitative traits, just use the total sample size for `n_eff`. sumstats$n_eff &lt;- 4 / (1 / 20791 + 1 / 323124) Note that we recommend to use imputed HapMap3+ variants when available, for which you can download some precomputed LD reference for European individuals based on the UK Biobank. Otherwise use the genotyped variants as I am doing here. Try to use an LD reference with at least 2000 individuals (I have only 1401 in this example). Please see this other tutorial for more information. Let us now match the variants in the GWAS summary statistics with the internal data we have: library(dplyr) info_snp &lt;- snp_match(sumstats, map, return_flip_and_rev = TRUE) %&gt;% mutate(freq = ifelse(`_REV_`, 1 - freq, freq), `_REV_` = NULL, `_FLIP_`= NULL) %&gt;% print() #&gt; chr pos a0 a1 beta beta_se freq info n_eff #&gt; 1 1 752566 T C 0.034083852 0.01445730 0.15920009 0.9975869 78136.41 #&gt; 2 1 785989 G A 0.018289010 0.01549153 0.13007399 0.9913233 78136.41 #&gt; 3 1 798959 G A 0.003331013 0.01307707 0.20524280 0.9734898 78136.41 #&gt; 4 1 947034 T C -0.021202725 0.02838148 0.03595249 0.9924989 78136.41 #&gt; _NUM_ID_.ss _NUM_ID_ #&gt; 1 2 2 #&gt; 2 4 4 #&gt; 3 5 5 #&gt; 4 6 6 #&gt; [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 340206 rows ] Check the summary statistics; some quality control may be needed: hist(info_snp$n_eff) # all the same values, otherwise filter at 70% of max hist(info_snp$info) # very good imputation; filter e.g. at 0.7 summary(info_snp$freq) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0000027 0.1100250 0.2261357 0.2365588 0.3580946 0.9998703 Then we can perform some quality control on the summary statistics by checking whether standard deviations inferred from the external GWAS summary statistics are consistent with the ones in the internal data we have: af_ref &lt;- big_colstats(G, ind.col = info_snp$`_NUM_ID_`, ncores = NCORES)$sum / (2 * nrow(G)) sd_ref &lt;- sqrt(2 * af_ref * (1 - af_ref)) sd_ss &lt;- with(info_snp, 2 / sqrt(n_eff * beta_se^2 + beta^2)) is_bad &lt;- sd_ss &lt; (0.5 * sd_ref) | sd_ss &gt; (sd_ref + 0.1) | sd_ss &lt; 0.05 | sd_ref &lt; 0.05 # basically filtering small MAF library(ggplot2) ggplot(slice_sample(data.frame(sd_ref, sd_ss, is_bad), n = 50e3)) + geom_point(aes(sd_ref, sd_ss, color = is_bad), alpha = 0.5) + theme_bigstatsr(0.9) + scale_color_viridis_d(direction = -1) + geom_abline(linetype = 2) + labs(x = &quot;Standard deviations in the reference set&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;, color = &quot;To remove?&quot;) When using quantitative traits (linear regression instead of logistic regression for the GWAS), you need to replace 2 by sd(y) when computing sd_ss (equations 1 and 2 of Privé, Arbel, Aschard, &amp; Vilhjálmsson (2022)). When allele frequencies are available in the GWAS summary statistics, you can use them (along with INFO scores) to get an even better match: sd_af &lt;- with(info_snp, sqrt(2 * freq * (1 - freq) * info)) ggplot(slice_sample(data.frame(sd_af, sd_ss), n = 50e3)) + geom_point(aes(sd_af, sd_ss), alpha = 0.5) + theme_bigstatsr(0.9) + geom_abline(linetype = 2, color = &quot;red&quot;) + labs(x = &quot;Standard deviations derived from allele frequencies&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;) You can still use the reference panel to do some quality control by comparing allele frequencies: diff &lt;- af_ref - info_snp$freq hist(diff, &quot;FD&quot;, xlim = c(-0.1, 0.1)) Then you can filter is_bad2 &lt;- sd_ss &lt; (0.7 * sd_af) | sd_ss &gt; (sd_af + 0.1) | sd_ss &lt; 0.05 | sd_af &lt; 0.05 | info_snp$info &lt; 0.7 | abs(diff) &gt; 0.07 mean(is_bad2) #&gt; [1] 0.002410276 table(is_bad, is_bad2) #&gt; is_bad2 #&gt; is_bad FALSE TRUE #&gt; FALSE 339138 410 #&gt; TRUE 252 410 df_beta &lt;- info_snp[!is_bad2, ] Then, we compute the correlation for each chromosome (note that we are using only 4 chromosomes for faster running of this tutorial): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- runonce::save_run({ ## indices in &#39;sumstats&#39; ind.chr &lt;- which(df_beta$chr == chr) ## indices in &#39;G&#39; ind.chr2 &lt;- df_beta$`_NUM_ID_`[ind.chr] POS2 &lt;- snp_asGeneticPos(map$chr[ind.chr2], map$pos[ind.chr2], dir = &quot;tmp-data&quot;) snp_cor(G, ind.col = ind.chr2, size = 3 / 1000, infos.pos = POS2, ncores = NCORES) }, file = paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) } #&gt; [1] 1 #&gt; user system elapsed #&gt; 36.72 0.42 13.09 #&gt; [1] 2 #&gt; user system elapsed #&gt; 48.61 0.29 14.96 #&gt; [1] 3 #&gt; user system elapsed #&gt; 45.52 0.17 13.65 #&gt; [1] 4 #&gt; user system elapsed #&gt; 38.14 0.05 11.25 Then we create the on-disk sparse genome-wide correlation matrix (again using only the first 4 chromosomes, for speed in this tutorial; replace by 1:22): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- readRDS(paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) if (chr == 1) { ld &lt;- Matrix::colSums(corr0^2) corr &lt;- as_SFBM(corr0, &quot;tmp-data/corr&quot;, compact = TRUE) } else { ld &lt;- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 To use the compact format for SFBMs, you need packageVersion(\"bigsparser\") &gt;= package_version(\"0.5\"). Make sure to reinstall {bigsnpr} when updating {bigsparser} to this new version (to avoid crashes). file.size(corr$sbk) / 1024^3 # file size in GB #&gt; [1] 0.5756225 Note that you will need at least the same memory as this file size (to keep it cached for faster processing) + some other memory for all the results returned. If you do not have enough memory, processing will be very slow (because you would read the data from disk all the time). If using HapMap3 variants, requesting 60 GB should be enough. For this small example, 8 GB of RAM should be enough. 7.1.2 LDpred2 We can now run LD score regression: df_beta &lt;- dplyr::filter(df_beta, chr %in% 1:4) # TO REMOVE (for speed here) (ldsc &lt;- with(df_beta, snp_ldsc(ld, length(ld), chi2 = (beta / beta_se)^2, sample_size = n_eff, blocks = NULL))) #&gt; int h2 #&gt; 0.9795999 0.0528327 ldsc_h2_est &lt;- ldsc[[&quot;h2&quot;]] We can now run LDpred2-inf very easily: # LDpred2-inf beta_inf &lt;- snp_ldpred2_inf(corr, df_beta, ldsc_h2_est) pred_inf &lt;- big_prodVec(G, beta_inf, ind.col = df_beta$`_NUM_ID_`) AUCBoot(pred_inf, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.55124053 0.51926895 0.58281738 0.01623797 For LDpred2(-grid), this is the grid we recommend to use: # LDpred2-grid (h2_seq &lt;- round(ldsc_h2_est * c(0.3, 0.7, 1, 1.4), 4)) #&gt; [1] 0.0158 0.0370 0.0528 0.0740 (p_seq &lt;- signif(seq_log(1e-5, 1, length.out = 21), 2)) #&gt; [1] 1.0e-05 1.8e-05 3.2e-05 5.6e-05 1.0e-04 1.8e-04 3.2e-04 5.6e-04 #&gt; [9] 1.0e-03 1.8e-03 3.2e-03 5.6e-03 1.0e-02 1.8e-02 3.2e-02 5.6e-02 #&gt; [17] 1.0e-01 1.8e-01 3.2e-01 5.6e-01 1.0e+00 params &lt;- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) dim(params) #&gt; [1] 168 3 Here, we will be using this smaller grid instead (for speed in this tutorial): (params &lt;- expand.grid(p = signif(seq_log(1e-4, 0.5, length.out = 16), 2), h2 = round(ldsc_h2_est, 4), sparse = TRUE)) #&gt; p h2 sparse #&gt; 1 0.00010 0.0528 TRUE #&gt; 2 0.00018 0.0528 TRUE #&gt; 3 0.00031 0.0528 TRUE #&gt; 4 0.00055 0.0528 TRUE #&gt; 5 0.00097 0.0528 TRUE #&gt; 6 0.00170 0.0528 TRUE #&gt; 7 0.00300 0.0528 TRUE #&gt; 8 0.00530 0.0528 TRUE #&gt; 9 0.00940 0.0528 TRUE #&gt; 10 0.01700 0.0528 TRUE #&gt; 11 0.02900 0.0528 TRUE #&gt; 12 0.05200 0.0528 TRUE #&gt; 13 0.09100 0.0528 TRUE #&gt; 14 0.16000 0.0528 TRUE #&gt; 15 0.28000 0.0528 TRUE #&gt; 16 0.50000 0.0528 TRUE beta_grid &lt;- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES) params$sparsity &lt;- colMeans(beta_grid == 0) Then, we can compute the corresponding PGS for all these models: pred_grid &lt;- big_prodMat(G, beta_grid, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params$score &lt;- apply(pred_grid, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) Note that missing values represent models that diverged substantially. ggplot(params, aes(x = p, y = score, color = as.factor(h2))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0), minor_breaks = params$p) + facet_wrap(~ sparse, labeller = label_both) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;h2&quot;) + theme(legend.position = &quot;top&quot;, panel.spacing = unit(1, &quot;lines&quot;)) Then you can use the best-performing model here. Note that you should use only individuals from the validation set to compute the $score and then evaluate the best model for the individuals in the test set. library(dplyr) best_beta_grid &lt;- params %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_grid[, .] To run LDpred2-auto, you can use: # LDpred2-auto multi_auto &lt;- snp_ldpred2_auto( corr, df_beta, h2_init = ldsc_h2_est, vec_p_init = seq_log(1e-4, 0.2, 30), burn_in = 100, num_iter = 100, # TO REMOVE, for speed here allow_jump_sign = FALSE, shrink_corr = 0.95, ncores = NCORES) Perform some quality control on the chains: # `range` should be between 0 and 2 (range &lt;- sapply(multi_auto, function(auto) diff(range(auto$corr_est)))) #&gt; [1] 0.05336403 0.05386446 0.05354641 0.05440121 0.05261745 0.05363724 #&gt; [7] 0.05486756 0.05479571 0.05401035 0.05412224 0.05294121 0.05130589 #&gt; [13] 0.05446501 0.05403041 0.05361384 0.05519109 0.05411719 0.05329519 #&gt; [19] 0.05275532 0.05251231 0.05339013 0.05332355 0.05476079 0.05375064 #&gt; [25] 0.05381365 0.05420337 0.05270925 0.05024719 0.05274067 0.05315884 (keep &lt;- (range &gt; (0.95 * quantile(range, 0.95)))) #&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #&gt; [12] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #&gt; [23] TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE To get the final effects / predictions, you should only use chains that pass this filtering: final_beta_auto &lt;- rowMeans(sapply(multi_auto[keep], function(auto) auto$beta_est)) We can finally test the final prediction final_pred_auto &lt;- big_prodVec(G, final_beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) AUCBoot(final_pred_auto, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.56633144 0.53428937 0.59807846 0.01638575 7.1.3 lassosum2: grid of models lassosum2 is a re-implementation of the lassosum model that now uses the exact same input parameters as LDpred2 (corr and df_beta). It can therefore be run next to LDpred2 and the best model can be chosen using the validation set. Note that parameter s from lassosum has been replaced by a new parameter delta in lassosum2, in order to better reflect that the lassosum model also uses L2-regularization (therefore, elastic-net regularization). beta_lassosum2 &lt;- snp_lassosum2( corr, df_beta, ncores = NCORES, nlambda = 10, maxiter = 50) # TO REMOVE, for speed here params2 &lt;- attr(beta_lassosum2, &quot;grid_param&quot;) pred_grid2 &lt;- big_prodMat(G, beta_lassosum2, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params2$score &lt;- apply(pred_grid2, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) ggplot(params2, aes(x = lambda, y = score, color = as.factor(delta))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0)) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;delta&quot;) best_grid_lassosum2 &lt;- params2 %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_lassosum2[, .] best_grid_overall &lt;- `if`(max(params2$score) &gt; max(params$score), best_grid_lassosum2, best_beta_grid) References Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K.,  others. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203209. Privé, F., Albiñana, C., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2022). Inferring disease architecture and predictive ability with LDpred2-auto. bioRxiv. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 54245431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 6574. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 12131221. "],["ipsych-data.html", "Chapter 8 iPSYCH data 8.1 Data on GenomeDK 8.2 Data on Statistics Denmark 8.3 Warnings about the data 8.4 Other data available", " Chapter 8 iPSYCH data 8.1 Data on GenomeDK The iPSYCH2015 data (Bybjerg-Grauholm et al., 2020) has been imputed (not by me) using the RICOPILI pipeline (Lam et al., 2020); you can get more info about this pipeline here imputation has been performed separately for the previous (2012) data and the new (2015) data outputs are in IMPUTE2 Oxford format storing imputation probabilities (to be 0, 1, or 2), separated for 2012 and 2015 and by regions of the genome RICOPILI qc1: variants with an INFO score &gt; 0.1 and MAF &gt; 0.005 Then, I transformed this data to my format by transforming probabilities to dosages, and merging all datasets while restricting to variants passing qc1 for both waves 2012 / 2015 (around 8.8M variants, across a total of around 134K individuals) this is available in my bigSNP format in sub-folder bigsnp_r_format/, which can be loaded into R with snp_attach(\"dosage_ipsych2015.rds\"), and for which the backingfile is one very large binary file of 1.1 TB this stores dosage data, i.e. expected genotype values \\(0 \\times P(0) + 1 \\times P(1) + 2 \\times P(2)\\) (between 0 and 2, but rounded to 2 decimal places through CODE_DOSAGE) there are also information on the individuals (in $fam) and on the variants (in $map) 8.2 Data on Statistics Denmark I split the dosage data (from my format) into 137 parts with at most 70K variants (also 4 parts for chromosome X), wrote these to text files, then Sussie converted these to SAS format to be sent to Statistics Denmark Emil helped convert these 137 SAS files back to my format the information on individuals was not sent to Statistics Denmark for some reason, but sample IDs are included (in $fam) so that information on individuals can be found elsewhere on the server and linked to the genotype data via these IDs (using e.g. match() or dplyr::left_join()) then you can either use my R packages to analyze the data or to write bed files (with a loss of information, where dosages are further rounded to 0/1/2) 8.3 Warnings about the data only dosage data is available on Statistics Denmark (i.e. imputation probabilities in the original format are not available there) imputation is far from perfect (due to small chips) imputation accuracies are not the same for 2012 / 2015, as well as for some allele frequencies; you may need to perform some QC, analyze the two cohorts separately, or at least add an indicator variable as covariate (is_2012, e.g. when performing a GWAS) 8.4 Other data available relatedness KING coefficients (\\(&gt; 2^{-4.5}\\)) computed between pairs of individuals (cf. section 4.1) PCs computed on the combined data, following best practices from Privé, Luu, Blum, McGrath, &amp; Vilhjálmsson (2020) (cf. chapter 5) a subset of homogeneous individuals (basically Northern Europeans) derived from PCs with two lines of code polygenic scores for 215 different traits and diseases, based on the UK Biobank individual-level data (Privé et al., 2022), computed for all iPSYCH individuals 900+ external polygenic scores derived by Clara from externally published GWAS summary statistics (Albinana et al., 2022) References Albinana, C., Zhu, Z., Schork, A.J., Ingason, A., Aschard, H., Brikell, I.,  others. (2022). Multi-PGS enhances polygenic prediction: Weighting 937 polygenic scores. medRxiv. Bybjerg-Grauholm, J., Pedersen, C.B., Bækvad-Hansen, M., Pedersen, M.G., Adamsen, D., Hansen, C.S.,  others. (2020). The iPSYCH2015 case-cohort sample: Updated directions for unravelling genetic and environmental architectures of severe mental disorders. medRxiv. Lam, M., Awasthi, S., Watson, H.J., Goldstein, J., Panagiotaropoulou, G., Trubetskoy, V.,  others. (2020). RICOPILI: Rapid imputation for COnsortias PIpeLIne. Bioinformatics, 36, 930933. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., OReilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 1223. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. "],["references.html", "References", " References Albinana, C., Zhu, Z., Schork, A.J., Ingason, A., Aschard, H., Brikell, I.,  others. (2022). Multi-PGS enhances polygenic prediction: Weighting 937 polygenic scores. medRxiv. Aschard, H., Vilhjálmsson, B.J., Joshi, A.D., Price, A.L., &amp; Kraft, P. (2015). Adjusting for heritable covariates can bias effect estimates in genome-wide association studies. The American Journal of Human Genetics, 96, 329339. Bengtsson, H. (2021). A unifying framework for parallel and distributed processing in R using futures. The R Journal, 13, 273291. Bybjerg-Grauholm, J., Pedersen, C.B., Bækvad-Hansen, M., Pedersen, M.G., Adamsen, D., Hansen, C.S.,  others. (2020). The iPSYCH2015 case-cohort sample: Updated directions for unravelling genetic and environmental architectures of severe mental disorders. medRxiv. Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L.T., Sharp, K.,  others. (2018). The UK Biobank resource with deep phenotyping and genomic data. Nature, 562, 203209. Chang, C.C., Chow, C.C., Tellier, L.C., Vattikuti, S., Purcell, S.M., &amp; Lee, J.J. (2015). Second-generation PLINK: Rising to the challenge of larger and richer datasets. Gigascience, 4, s13742015. Lam, M., Awasthi, S., Watson, H.J., Goldstein, J., Panagiotaropoulou, G., Trubetskoy, V.,  others. (2020). RICOPILI: Rapid imputation for COnsortias PIpeLIne. Bioinformatics, 36, 930933. Manichaikul, A., Mychaleckyj, J.C., Rich, S.S., Daly, K., Sale, M., &amp; Chen, W.-M. (2010). Robust relationship inference in genome-wide association studies. Bioinformatics, 26, 28672873. Privé, F. (2021). Optimal linkage disequilibrium splitting. Bioinformatics, 38, 255256. Privé, F. (2022). Using the UK Biobank as a global reference of worldwide populations: application to measuring ancestry diversity from GWAS summary statistics. Bioinformatics, 38, 34773480. Privé, F., Albiñana, C., Pasaniuc, B., &amp; Vilhjálmsson, B.J. (2022). Inferring disease architecture and predictive ability with LDpred2-auto. bioRxiv. Privé, F., Arbel, J., Aschard, H., &amp; Vilhjálmsson, B.J. (2022). Identifying and correcting for misspecifications in GWAS summary statistics and polygenic scores. Human Genetics and Genomics Advances. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 54245431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 6574. Privé, F., Aschard, H., Carmi, S., Folkersen, L., Hoggart, C., OReilly, P.F., &amp; Vilhjálmsson, B.J. (2022). Portability of 245 polygenic scores when derived from the UK Biobank and applied to 9 ancestry groups from the same cohort. The American Journal of Human Genetics, 109, 1223. Privé, F., Aschard, H., Ziyatdinov, A., &amp; Blum, M.G.B. (2018). Efficient analysis of large-scale genome-wide data with two R packages: bigstatsr and bigsnpr. Bioinformatics, 34, 27812787. Privé, F., Luu, K., Blum, M.G., McGrath, J.J., &amp; Vilhjálmsson, B.J. (2020). Efficient toolkit implementing best practices for principal component analysis of population genetic data. Bioinformatics, 36, 44494457. Privé, F., Luu, K., Vilhjálmsson, B.J., &amp; Blum, M.G. (2020). Performing highly efficient genome scans for local adaptation with R package pcadapt version 4. Molecular Biology and Evolution, 37, 21532154. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 12131221. Reed, E., Nunez, S., Kulp, D., Qian, J., Reilly, M.P., &amp; Foulkes, A.S. (2015). A guide to genome-wide association analysis and post-analytic interrogation. Statistics in Medicine, 34, 37693792. Visscher, P.M., Wray, N.R., Zhang, Q., Sklar, P., McCarthy, M.I., Brown, M.A., &amp; Yang, J. (2017). 10 years of GWAS discovery: Biology, function, and translation. The American Journal of Human Genetics, 101, 522. Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L.D., François, R.,  others. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4, 1686. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
