[["polygenic-scores-pgs.html", "Chapter 6 Polygenic scores (PGS) 6.1 Exercises: LDpred2 and lassosum2", " Chapter 6 Polygenic scores (PGS) Polygenic scores are one of the main focus of package {bigsnpr}. These are the main methods currently available: penalized regressions, with individual-level data (Privé, Aschard, and Blum 2019) [tutorial] Clumping and Thresholding (C+T) and Stacked C+T (SCT), with summary statistics and individual level data (Privé et al. 2019) [tutorial] LDpred2, with summary statistics (Privé, Arbel, and Vilhjálmsson 2020) [tutorial] lassosum2, with the same input data as LDpred2 (coming soon) 6.1 Exercises: LDpred2 and lassosum2 You should also check the other tutorials mentioned before. 6.1.1 Preparing the data Let us first read the data produced in 3.3: library(bigsnpr) obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes NCORES &lt;- nb_cores() map &lt;- dplyr::transmute(obj.bigsnp$map, chr = chromosome, pos = physical.pos, a0 = allele2, a1 = allele1) We also get the phenotype data: url &lt;- &quot;https://www.mtholyoke.edu/courses/afoulkes/Data/statsTeachR/&quot; clinical &lt;- bigreadr::fread2( runonce::download_file(paste0(url, &quot;GWAS_clinical.csv&quot;), dir = &quot;tmp-data&quot;)) # get the same order as for the genotypes # (to match over multiple columns, use `vctrs::vec_match()`) info &lt;- clinical[match(obj.bigsnp$fam$family.ID, clinical$FamID), ] We then download some GWAS summary statistics from a large consortium (Nikpay et al. 2015; Buniello et al. 2019) and prepare them in the format required by LDpred2: txt &lt;- runonce::download_file( &quot;ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/NikpayM_26343387_GCST003116/cad.add.160614.website.txt&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD.txt&quot;) sumstats &lt;- bigreadr::fread2( txt, select = c(&quot;chr&quot;, &quot;bp_hg19&quot;, &quot;noneffect_allele&quot;, &quot;effect_allele&quot;, &quot;beta&quot;, &quot;se_dgc&quot;), col.names = c(&quot;chr&quot;, &quot;pos&quot;, &quot;a0&quot;, &quot;a1&quot;, &quot;beta&quot;, &quot;beta_se&quot;)) # GWAS effective sample size from the paper # would better to have a per-variant sample size in the sumstats sumstats$n_eff &lt;- 4 / (1 / 60801 + 1 / 123504) Let us now match these with the internal data we have: info_snp &lt;- snp_match(sumstats, map) #&gt; 9,455,778 variants to be matched. #&gt; 62,531 ambiguous SNPs have been removed. #&gt; 340,563 variants have been matched; 170,352 were flipped and 309,568 were reversed. (info_snp &lt;- tibble::as_tibble(info_snp)) #&gt; # A tibble: 340,563 x 9 #&gt; chr pos a0 a1 beta beta_se n_eff `_NUM_ID_.ss` #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 7.53e5 T C 5.24e-3 0.0158 1.63e5 2 #&gt; 2 1 7.86e5 G A -9.81e-4 0.0163 1.63e5 54 #&gt; 3 1 7.99e5 G A 1.56e-2 0.0143 1.63e5 80 #&gt; 4 1 9.50e5 G A -4.01e-2 0.0134 1.63e5 126 #&gt; 5 1 1.02e6 G A -1.50e-2 0.0101 1.63e5 246 #&gt; 6 1 1.04e6 T C -8.94e-3 0.0130 1.63e5 309 #&gt; 7 1 1.13e6 C A 7.63e-3 0.0147 1.63e5 584 #&gt; 8 1 1.13e6 A C 1.43e-3 0.0164 1.63e5 594 #&gt; 9 1 1.16e6 C T -2.21e-2 0.0208 1.63e5 719 #&gt; 10 1 1.17e6 C T 2.78e-2 0.0207 1.63e5 776 #&gt; # ... with 340,553 more rows, and 1 more variable: `_NUM_ID_` &lt;int&gt; Note that we recommend to use imputed HapMap3 variants when available, otherwise you can just use the genotyped variants as I am doing here. Then we can perform some quality control on the summary statistics to see how standard deviations inferred from the external GWAS summary statistics are consistent with the ones in the internal data we have: sd_val &lt;- sqrt(big_colstats(G, ind.col = info_snp$`_NUM_ID_`, ncores = NCORES)$var) sd_ss &lt;- with(info_snp, 2 / sqrt(n_eff * beta_se^2 + beta^2)) is_bad &lt;- sd_ss &lt; (0.5 * sd_val) | sd_ss &gt; (sd_val + 0.1) | sd_ss &lt; 0.1 | sd_val &lt; 0.05 library(ggplot2) ggplot(dplyr::slice_sample(data.frame(sd_val, sd_ss, is_bad), n = 50e3)) + geom_point(aes(sd_val, sd_ss, color = is_bad), alpha = 0.5) + theme_bigstatsr() + scale_color_viridis_d(direction = -1) + geom_abline(linetype = 2) + labs(x = &quot;Standard deviations in the validation set&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;, color = &quot;To remove?&quot;) df_beta &lt;- info_snp[!is_bad, ] Then, we compute the correlation for each chromosome (note that we are using only 4 chromosomes for faster running of this tutorial): for (chr in 1:4) { corr0 &lt;- runonce::save_run({ print(chr) ## indices in &#39;sumstats&#39; ind.chr &lt;- which(df_beta$chr == chr) ## indices in &#39;G&#39; ind.chr2 &lt;- df_beta$`_NUM_ID_`[ind.chr] POS2 &lt;- snp_asGeneticPos(map$chr[ind.chr2], map$pos[ind.chr2], dir = &quot;tmp-data&quot;) snp_cor(G, ind.col = ind.chr2, size = 3 / 1000, infos.pos = POS2, ncores = NCORES) }, file = paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) } Then we create the on-disk sparse genome-wide correlation matrix (again using only the first 4 chromosomes, for speed in this tutorial; replace by 1:22): for (chr in 1:4) { print(chr) corr0 &lt;- readRDS(paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) if (chr == 1) { ld &lt;- Matrix::colSums(corr0^2) corr &lt;- as_SFBM(corr0, &quot;tmp-data/corr&quot;) } else { ld &lt;- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 6.1.2 LDpred2 We can now run LD score regression: df_beta &lt;- dplyr::filter(df_beta, chr %in% 1:4) # TO REMOVE (for speed here) (ldsc &lt;- with(df_beta, snp_ldsc(ld, length(ld), chi2 = (beta / beta_se)^2, sample_size = n_eff, blocks = NULL))) #&gt; int h2 #&gt; 0.87323243 0.02092089 ldsc_h2_est &lt;- ldsc[[&quot;h2&quot;]] We can now run LDpred2-inf very easily: # LDpred2-inf beta_inf &lt;- snp_ldpred2_inf(corr, df_beta, ldsc_h2_est) pred_inf &lt;- big_prodVec(G, beta_inf, ind.col = df_beta$`_NUM_ID_`) AUCBoot(pred_inf, info$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.64428351 0.61335824 0.67504990 0.01573054 For LDpred2(-grid), this is the grid we recommend to use: # LDpred2-grid (h2_seq &lt;- round(ldsc_h2_est * c(0.7, 1, 1.4), 4)) #&gt; [1] 0.0146 0.0209 0.0293 (p_seq &lt;- signif(seq_log(1e-5, 1, length.out = 21), 2)) #&gt; [1] 1.0e-05 1.8e-05 3.2e-05 5.6e-05 1.0e-04 1.8e-04 3.2e-04 5.6e-04 #&gt; [9] 1.0e-03 1.8e-03 3.2e-03 5.6e-03 1.0e-02 1.8e-02 3.2e-02 5.6e-02 #&gt; [17] 1.0e-01 1.8e-01 3.2e-01 5.6e-01 1.0e+00 params &lt;- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) dim(params) #&gt; [1] 126 3 Here, we will be using this smaller grid instead (for speed in this tutorial): (params &lt;- expand.grid(p = signif(seq_log(0.001, 0.2, length.out = 8), 2), h2 = round(ldsc_h2_est, 4), sparse = TRUE)) #&gt; p h2 sparse #&gt; 1 0.0010 0.0209 TRUE #&gt; 2 0.0021 0.0209 TRUE #&gt; 3 0.0045 0.0209 TRUE #&gt; 4 0.0097 0.0209 TRUE #&gt; 5 0.0210 0.0209 TRUE #&gt; 6 0.0440 0.0209 TRUE #&gt; 7 0.0940 0.0209 TRUE #&gt; 8 0.2000 0.0209 TRUE beta_grid &lt;- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES) params$sparsity &lt;- colMeans(beta_grid == 0) Then, we can compute the corresponding PGS for all these models: pred_grid &lt;- big_prodMat(G, beta_grid, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params$score &lt;- big_univLogReg(as_FBM(pred_grid), info$CAD)$score ggplot(params, aes(x = p, y = score, color = as.factor(h2))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0), minor_breaks = params$p) + facet_wrap(~ sparse, labeller = label_both) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;h2&quot;) + theme(legend.position = &quot;top&quot;, panel.spacing = unit(1, &quot;lines&quot;)) Then you can use the best-performing model here (note that you should use only individuals from the validation set to compute the $score and then evaluate the best model for the individuals in the test set). library(dplyr) #&gt; Warning: package &#39;dplyr&#39; was built under R version 3.6.3 best_beta_grid &lt;- params %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_grid[, .] To run LDpred2-auto, you can use: # LDpred2-auto # Note: use vec_p_init = seq_log(1e-4, 0.5, 30) in real scenario # and defaults for &#39;burn_in&#39; and &#39;num_iter&#39; (again, for speed here..) multi_auto &lt;- snp_ldpred2_auto( corr, df_beta, h2_init = ldsc_h2_est, vec_p_init = seq_log(1e-3, 0.1, 4), # TO CHANGE burn_in = 50, num_iter = 50, # TO REMOVE ncores = NCORES) We should perform some quality control on the chains (you should have more than 4, e.g. 30): beta_auto &lt;- sapply(multi_auto, function(auto) auto$beta_est) pred_auto &lt;- big_prodMat(G, beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) sc &lt;- apply(pred_auto, 2, sd) keep &lt;- abs(sc - median(sc)) &lt; 3 * mad(sc) final_beta_auto &lt;- rowMeans(beta_auto[, keep]) We can finally test the final prediction # could directly use `rowMeans(pred_auto[, keep])` here final_pred_auto &lt;- big_prodVec(G, final_beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) AUCBoot(final_pred_auto, info$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.61304403 0.58190696 0.64429158 0.01594833 6.1.3 lassosum2: grid of models lassosum2 is a re-implementation of the lassosum model that now uses the exact same input parameters as LDpred2 (corr and df_beta). It should be much faster than running LDpred2. It can be run next to LDpred2 and the best model can be chosen using the validation set. beta_lassosum2 &lt;- snp_lassosum2( corr, df_beta, ncores = NCORES, s = c(0.2, 0.5, 0.8, 1), nlambda = 10, maxiter = 50) # TO REMOVE params2 &lt;- attr(beta_lassosum2, &quot;grid_param&quot;) pred_grid2 &lt;- big_prodMat(G, beta_lassosum2, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params2$score &lt;- big_univLogReg(as_FBM(pred_grid2), info$CAD)$score library(ggplot2) ggplot(params2, aes(x = lambda, y = score, color = as.factor(s))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0)) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;s&quot;) library(dplyr) best_grid_lassosum2 &lt;- params2 %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_lassosum2[, .] best_grid_overall &lt;- `if`(max(params2$score) &gt; max(params$score), best_grid_lassosum2, best_beta_grid) "]]
