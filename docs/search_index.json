[["polygenic-scores-pgs.html", "Chapter 6 Polygenic scores (PGS) 6.1 Exercises: LDpred2 and lassosum2", " Chapter 6 Polygenic scores (PGS) Polygenic scores are one of the main focus of package {bigsnpr}. These are the main methods currently available: penalized regressions, with individual-level data (Privé, Aschard, &amp; Blum, 2019) [tutorial] Clumping and Thresholding (C+T) and Stacked C+T (SCT), with summary statistics and individual level data (Privé, Vilhjálmsson, Aschard, &amp; Blum, 2019) [tutorial] LDpred2, with summary statistics (Privé, Arbel, &amp; Vilhjálmsson, 2020) [tutorial] lassosum2, with the same input data as LDpred2 6.1 Exercises: LDpred2 and lassosum2 You should also check the other tutorials mentioned before. 6.1.1 Preparing the data Let us first read the data produced in 3.3: library(bigsnpr) obj.bigsnp &lt;- snp_attach(&quot;tmp-data/GWAS_data_sorted_QC.rds&quot;) G &lt;- obj.bigsnp$genotypes NCORES &lt;- nb_cores() map &lt;- dplyr::transmute(obj.bigsnp$map, chr = chromosome, pos = physical.pos, a0 = allele2, a1 = allele1) We then download some GWAS summary statistics from a large consortium (Buniello et al., 2019; Nikpay et al., 2015) and prepare them in the format required by LDpred2: txt &lt;- runonce::download_file( &quot;http://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST003001-GCST004000/GCST003116/cad.add.160614.website.txt&quot;, dir = &quot;tmp-data&quot;, fname = &quot;sumstats_CAD.txt&quot;) writeLines(readLines(txt, n = 3)) #&gt; markername chr bp_hg19 effect_allele noneffect_allele effect_allele_freq median_info model beta se_dgc p_dgc het_pvalue n_studies #&gt; rs143225517 1 751756 C T .158264 .92 FIXED .013006 .017324 .4528019 .303481 35 #&gt; rs3094315 1 752566 A G .763018 1 FIXED -.005243 .0157652 .7394597 .146867 36 # One could also read other variables such as &#39;median_info&#39;, &#39;model&#39;, # &#39;het_pvalue&#39;, and &#39;n_studies&#39; to apply some quality control to the sumstats. sumstats &lt;- bigreadr::fread2( txt, select = c(&quot;chr&quot;, &quot;bp_hg19&quot;, &quot;noneffect_allele&quot;, &quot;effect_allele&quot;, &quot;beta&quot;, &quot;se_dgc&quot;), col.names = c(&quot;chr&quot;, &quot;pos&quot;, &quot;a0&quot;, &quot;a1&quot;, &quot;beta&quot;, &quot;beta_se&quot;)) # GWAS effective sample size from the paper; # it would be better to have a per-variant sample size in the sumstats. # For quantitative traits, just use the total sample size for `n_eff`. sumstats$n_eff &lt;- 4 / (1 / 60801 + 1 / 123504) Let us now match these with the internal data we have: (info_snp &lt;- tibble::as_tibble(snp_match(sumstats, map))) #&gt; 9,455,778 variants to be matched. #&gt; 62,531 ambiguous SNPs have been removed. #&gt; 340,563 variants have been matched; 170,352 were flipped and 309,568 were reversed. #&gt; # A tibble: 340,563 x 9 #&gt; chr pos a0 a1 beta beta_se n_eff `_NUM_ID_.ss` #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 752566 T C 0.00524 0.0158 162973. 2 #&gt; 2 1 785989 G A -0.000981 0.0163 162973. 54 #&gt; 3 1 798959 G A 0.0156 0.0143 162973. 80 #&gt; 4 1 949608 G A -0.0401 0.0134 162973. 126 #&gt; 5 1 1018704 G A -0.0150 0.0101 162973. 246 #&gt; 6 1 1041700 T C -0.00894 0.0130 162973. 309 #&gt; 7 1 1129672 C A 0.00763 0.0147 162973. 584 #&gt; 8 1 1130727 A C 0.00143 0.0164 162973. 594 #&gt; 9 1 1158631 C T -0.0221 0.0208 162973. 719 #&gt; 10 1 1165310 C T 0.0278 0.0207 162973. 776 #&gt; # ... with 340,553 more rows, and 1 more variable: _NUM_ID_ &lt;int&gt; Note that we recommend to use imputed HapMap3 variants when available, otherwise you can just use the genotyped variants as I am doing here. Try to use an LD reference with at least 2000 individuals (I have only 1401 in this example). You can download some precomputed LD reference for European individuals based on the UK Biobank (+ some example R script and some information on the HapMap3 variants) at https://figshare.com/articles/dataset/European_LD_reference/13034123. Then we can perform some quality control on the summary statistics to see how standard deviations inferred from the external GWAS summary statistics are consistent with the ones in the internal data we have: maf &lt;- snp_MAF(G, ind.col = info_snp$`_NUM_ID_`, ncores = NCORES) sd_val &lt;- sqrt(2 * maf * (1 - maf)) sd_ss &lt;- with(info_snp, 2 / sqrt(n_eff * beta_se^2 + beta^2)) is_bad &lt;- sd_ss &lt; (0.5 * sd_val) | sd_ss &gt; (sd_val + 0.1) | sd_ss &lt; 0.1 | sd_val &lt; 0.05 When using quantitative traits (linear regression instead of logistic regression for the GWAS), you need to replace 2 by sd(y) when computing sd_ss. Have a look at section 3.4 of the LDpred2 paper. library(ggplot2) ggplot(dplyr::slice_sample(data.frame(sd_val, sd_ss, is_bad), n = 50e3)) + geom_point(aes(sd_val, sd_ss, color = is_bad), alpha = 0.5) + theme_bigstatsr(0.9) + scale_color_viridis_d(direction = -1) + geom_abline(linetype = 2) + labs(x = &quot;Standard deviations in the validation set&quot;, y = &quot;Standard deviations derived from the summary statistics&quot;, color = &quot;To remove?&quot;, title = &quot;This is far from perfect; Neff is probably overestimated.&quot;) df_beta &lt;- info_snp[!is_bad, ] Then, we compute the correlation for each chromosome (note that we are using only 4 chromosomes for faster running of this tutorial): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- runonce::save_run({ ## indices in &#39;sumstats&#39; ind.chr &lt;- which(df_beta$chr == chr) ## indices in &#39;G&#39; ind.chr2 &lt;- df_beta$`_NUM_ID_`[ind.chr] POS2 &lt;- snp_asGeneticPos(map$chr[ind.chr2], map$pos[ind.chr2], dir = &quot;tmp-data&quot;) snp_cor(G, ind.col = ind.chr2, size = 3 / 1000, infos.pos = POS2, ncores = NCORES) }, file = paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) } #&gt; [1] 1 #&gt; user system elapsed #&gt; 61.94 1.11 39.75 #&gt; [1] 2 #&gt; user system elapsed #&gt; 78.23 1.39 55.51 #&gt; [1] 3 #&gt; user system elapsed #&gt; 79.24 1.04 52.75 #&gt; [1] 4 #&gt; user system elapsed #&gt; 53.17 0.74 35.75 Then we create the on-disk sparse genome-wide correlation matrix (again using only the first 4 chromosomes, for speed in this tutorial; replace by 1:22): for (chr in 1:4) { # REPLACE BY 1:22 print(chr) corr0 &lt;- readRDS(paste0(&quot;tmp-data/corr_chr&quot;, chr, &quot;.rds&quot;)) if (chr == 1) { ld &lt;- Matrix::colSums(corr0^2) corr &lt;- as_SFBM(corr0, &quot;tmp-data/corr&quot;, compact = TRUE) } else { ld &lt;- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 Note that the compact format for SFBMs is quite new. You will need packageVersion(\"bigsparser\") &gt;= package_version(\"0.5\"). Make sure to reinstall {bigsnpr} when updating {bigsparser} to this new version (to avoid crashes). file.size(corr$sbk) / 1024^3 # file size in GB #&gt; [1] 0.5774516 Note that you will need at least the same memory as this file size (to keep it cached for faster processing) + some other memory for all the results returned. If you do not have enough memory, processing will be very slow (because you would read the data from disk all the time). If using HapMap3 variants, requesting 60 GB should be enough. For this small example, 8 GB of RAM should be enough. 6.1.2 LDpred2 We can now run LD score regression: df_beta &lt;- dplyr::filter(df_beta, chr %in% 1:4) # TO REMOVE (for speed here) (ldsc &lt;- with(df_beta, snp_ldsc(ld, length(ld), chi2 = (beta / beta_se)^2, sample_size = n_eff, blocks = NULL))) #&gt; int h2 #&gt; 0.87242475 0.02097793 ldsc_h2_est &lt;- ldsc[[&quot;h2&quot;]] We can now run LDpred2-inf very easily: # LDpred2-inf beta_inf &lt;- snp_ldpred2_inf(corr, df_beta, ldsc_h2_est) pred_inf &lt;- big_prodVec(G, beta_inf, ind.col = df_beta$`_NUM_ID_`) AUCBoot(pred_inf, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.64455819 0.61412134 0.67514452 0.01557294 For LDpred2(-grid), this is the grid we recommend to use: # LDpred2-grid (h2_seq &lt;- round(ldsc_h2_est * c(0.3, 0.7, 1, 1.4), 4)) #&gt; [1] 0.0063 0.0147 0.0210 0.0294 (p_seq &lt;- signif(seq_log(1e-5, 1, length.out = 21), 2)) #&gt; [1] 1.0e-05 1.8e-05 3.2e-05 5.6e-05 1.0e-04 1.8e-04 3.2e-04 5.6e-04 #&gt; [9] 1.0e-03 1.8e-03 3.2e-03 5.6e-03 1.0e-02 1.8e-02 3.2e-02 5.6e-02 #&gt; [17] 1.0e-01 1.8e-01 3.2e-01 5.6e-01 1.0e+00 params &lt;- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) dim(params) #&gt; [1] 168 3 Here, we will be using this smaller grid instead (for speed in this tutorial): (params &lt;- expand.grid(p = signif(seq_log(0.001, 0.3, length.out = 8), 2), h2 = round(ldsc_h2_est, 4), sparse = TRUE)) #&gt; p h2 sparse #&gt; 1 0.0010 0.021 TRUE #&gt; 2 0.0023 0.021 TRUE #&gt; 3 0.0051 0.021 TRUE #&gt; 4 0.0120 0.021 TRUE #&gt; 5 0.0260 0.021 TRUE #&gt; 6 0.0590 0.021 TRUE #&gt; 7 0.1300 0.021 TRUE #&gt; 8 0.3000 0.021 TRUE beta_grid &lt;- snp_ldpred2_grid(corr, df_beta, params, ncores = NCORES) params$sparsity &lt;- colMeans(beta_grid == 0) Then, we can compute the corresponding PGS for all these models: pred_grid &lt;- big_prodMat(G, beta_grid, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params$score &lt;- apply(pred_grid, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) Note that missing values represent models that diverged substantially. ggplot(params, aes(x = p, y = score, color = as.factor(h2))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0), minor_breaks = params$p) + facet_wrap(~ sparse, labeller = label_both) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;h2&quot;) + theme(legend.position = &quot;top&quot;, panel.spacing = unit(1, &quot;lines&quot;)) Then you can use the best-performing model here. Note that you should use only individuals from the validation set to compute the $score and then evaluate the best model for the individuals in the test set. library(dplyr) #&gt; Warning: package &#39;dplyr&#39; was built under R version 3.6.3 best_beta_grid &lt;- params %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_grid[, .] To run LDpred2-auto, you can use: # LDpred2-auto # Note: use vec_p_init = seq_log(1e-4, 0.5, 30) in real scenario # and defaults for &#39;burn_in&#39; and &#39;num_iter&#39; (again, for speed here) multi_auto &lt;- snp_ldpred2_auto( corr, df_beta, h2_init = ldsc_h2_est, vec_p_init = seq_log(1e-3, 0.1, NCORES), # TO CHANGE burn_in = 50, num_iter = 50, # TO REMOVE ncores = NCORES) We should perform some quality control on the chains (you should have more than 4, e.g. 30): beta_auto &lt;- sapply(multi_auto, function(auto) auto$beta_est) pred_auto &lt;- big_prodMat(G, beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) sc &lt;- apply(pred_auto, 2, sd) keep &lt;- abs(sc - median(sc)) &lt; 3 * mad(sc) final_beta_auto &lt;- rowMeans(beta_auto[, keep]) We can finally test the final prediction # could directly use `rowMeans(pred_auto[, keep])` here final_pred_auto &lt;- big_prodVec(G, final_beta_auto, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) AUCBoot(final_pred_auto, obj.bigsnp$fam$CAD) #&gt; Mean 2.5% 97.5% Sd #&gt; 0.60882985 0.57735684 0.63952135 0.01587818 6.1.3 lassosum2: grid of models lassosum2 is a re-implementation of the lassosum model that now uses the exact same input parameters as LDpred2 (corr and df_beta). It should be fast to run. It can be run next to LDpred2 and the best model can be chosen using the validation set. Note that parameter s from lassosum has been replaced by a new parameter delta in lassosum2, in order to better reflect that the lassosum model also uses L2-regularization (therefore, elastic-net regularization). beta_lassosum2 &lt;- snp_lassosum2( corr, df_beta, ncores = NCORES, nlambda = 10, maxiter = 50) # TO REMOVE params2 &lt;- attr(beta_lassosum2, &quot;grid_param&quot;) pred_grid2 &lt;- big_prodMat(G, beta_lassosum2, ind.col = df_beta[[&quot;_NUM_ID_&quot;]], ncores = NCORES) params2$score &lt;- apply(pred_grid2, 2, function(x) { if (all(is.na(x))) return(NA) summary(glm(CAD ~ x + sex + age, data = obj.bigsnp$fam, family = &quot;binomial&quot;))$coef[&quot;x&quot;, 3] }) library(ggplot2) ggplot(params2, aes(x = lambda, y = score, color = as.factor(delta))) + theme_bigstatsr() + geom_point() + geom_line() + scale_x_log10(breaks = 10^(-5:0)) + labs(y = &quot;GLM Z-Score&quot;, color = &quot;delta&quot;) library(dplyr) best_grid_lassosum2 &lt;- params2 %&gt;% mutate(id = row_number()) %&gt;% arrange(desc(score)) %&gt;% slice(1) %&gt;% pull(id) %&gt;% beta_lassosum2[, .] best_grid_overall &lt;- `if`(max(params2$score) &gt; max(params$score), best_grid_lassosum2, best_beta_grid) Buniello, A., MacArthur, J.A.L., Cerezo, M., Harris, L.W., Hayhurst, J., Malangone, C.,  others. (2019). The NHGRI-EBI GWAS Catalog of published genome-wide association studies, targeted arrays and summary statistics 2019. Nucleic Acids Research, 47, D1005D1012. Nikpay, M., Goel, A., Won, H.-H., Hall, L.M., Willenborg, C., Kanoni, S.,  others. (2015). A comprehensive 1000 genomesbased genome-wide association meta-analysis of coronary artery disease. Nature Genetics, 47, 1121. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 54245431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 6574. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 12131221. References Buniello, A., MacArthur, J.A.L., Cerezo, M., Harris, L.W., Hayhurst, J., Malangone, C.,  others. (2019). The NHGRI-EBI GWAS Catalog of published genome-wide association studies, targeted arrays and summary statistics 2019. Nucleic Acids Research, 47, D1005D1012. Nikpay, M., Goel, A., Won, H.-H., Hall, L.M., Willenborg, C., Kanoni, S.,  others. (2015). A comprehensive 1000 genomesbased genome-wide association meta-analysis of coronary artery disease. Nature Genetics, 47, 1121. Privé, F., Arbel, J., &amp; Vilhjálmsson, B.J. (2020). LDpred2: better, faster, stronger. Bioinformatics, 36, 54245431. Privé, F., Aschard, H., &amp; Blum, M.G. (2019). Efficient implementation of penalized regression for genetic risk prediction. Genetics, 212, 6574. Privé, F., Vilhjálmsson, B.J., Aschard, H., &amp; Blum, M.G.B. (2019). Making the most of clumping and thresholding for polygenic scores. The American Journal of Human Genetics, 105, 12131221. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
